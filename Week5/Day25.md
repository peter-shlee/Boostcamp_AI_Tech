# Day 25 - 그래프 신경망

## 그래프 신경망이란 무엇일까? (기본)

### 기본적인 그래프 신경망

* 그래프 신경망(graph neural network)은 귀납식 (정점 or 그래프) 임베딩 방법이다
* 출력으로 인코더를 얻는 귀납식 임베딩 방법은 여러 장점을 갖는다
* 학습이 진행된 이후에 추가된 정점에 대해서도 임베딩을 얻을 수 있다
* 모든 정점에 대한 임베딩을 미리 계산하여 저장해둘 필요가 없다
* 정점이 가진 속성 정보를 활용할 수 있다

#### 그래프 신경망 구조

* 그래프 신경망은 그래프(인접 행렬)와 정점의 속성 정보(속성 벡터, attribute vector)를 입력으로 받는다
* 정점 속성 벡터는 m차원 벡터이고, 여기서 m은 속성의 수를 의미한다
* 정점의 속성의 예신느 다음과 같다
  * sns 사용자(정점)의 성별, 연령, 사는 지역 등
  * 논문 인용 그래프에서 논문(정점)에 사용된 키워드에 대한 one-hot vector
  * pageRank등의 정점 중심성, 군집 계수 등

* 그래프 신경망은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻는다
* 대상 정점의 임베딩을 얻기 위해 이웃들 그리고 이웃의 이웃들의 정보... 를 집계한다
* 각 집계 단계를 layer로 구분하고, 각 layer의 각 정점마다 embedding을 얻는다  
        ![그래프 신경망](./img/day25/gnn1.png)
* 따라서 각 정점마다 집계되는 정보가 상이하다  
        ![그래프 신경망](./img/day25/gnn2.png)
* layer 내의 정점들 간에는 집계 함수를 공유한다 **(하나의 layer 안에서는 동일한 가중치 사용)**  
        ![그래프 신경망](./img/day25/gnn3.png)
* 정점마다 degree가 다르므로 집계 함수는 가변적인 input을 처리할 수 있어야 한다
* 따라서 집계 함수는 이웃 정점들의 embedding vector의 평균을 이용한다  
        ![그래프 신경망](./img/day25/gnn4.png)
* 집계 함수의 수식은 다음과 같다  
        ![그래프 신경망](./img/day25/gnn5.png)
* 여기서 학습해야 하는 parameter는 $W_k$와 $B_k$이고, 이는 층 별 신경망의 가중치임
* 이전 층에서 정점 v의 임베딩을 추가로 더해주는 것은 residual connection과 유사한 역할일 것이라고 추측함
* 이웃의 이웃까지 고려한다고 가정한다면 위쪽의 그래프에서 A의 embedding vector를 구하는 과정은 다음과 같다
  * layer 0
    * A, B, C, E, F 정점의 attribute vector로 $h^0_A$, $h^0_B$, $h^0_C$, $h^0_E$, $h^0_F$를 초기화
  * layer 1
    * $h^1_B$ - $h^0_A$, $h^0_C$의 평균과 $h^0_B$를 이용하여 이용하여 구한다
    * $h^1_C$ - $h^0_A$, $h^0_B$, $h^0_E$, $h^0_F$의 평균과 $h^0_C$를 이용하여 구한다
    * $h^1_D$ - $h^0_A$와 $h^0_D$를 이용하여 구한다
  * layer 2
    * $h^2_A$ - $h^1_B$, $h^1_C$, $h^1_D$의 평균과 $h^1_A$을 이용하여 구한다. 이 때 $h^1_A$는 기존에 계산해놓지 않았으므로 $h^0_B$, $h^0_C$, $h^0_D$의 평균을 이용해서 추가적으로 구해야 한다.
  * 최종적으로 나온 $h^2_A$가 정점 A의 embedding vector가 된다  

#### 그래프 신경망의 학습

* 위 수식의 $W_k$와 $B_k$가 그래프 신경망의 parameter이다
* 학습을 위해 loss function을 정한다
* 변환식 정점 임베딩에서 처럼 그래프에서의 정점간 거리를 보존하는 것을 목표로 할 수 있다
* 이 때 인접성을 기반으로 유사도를 정의한다면 손실 함수는 다음과 같다  
        ![그래프 신경망](./img/day25/gnn6.png)
* 후속 과제(downstream task)의 loss function을 이용한 end to end 학습도 가능하다
* 정점 분류가 최종 목표인 경우를 예로 들면 다음과 같다
* 그래프 신경망을 이용하여 정점의 embedding을 구한 뒤, 정점의 embedding을 분류기(classifier)의 입력으로 넣고, 분류된 결과를 얻어야 한다
* 이 경우 classifier의 손실함수인 교차 엔트로피(cross entropy)를 전체 프로세스의 손실함수로 사용하여 end to end 학습을 할 수 있다
* 이 경우 loss function의 수식은 다음과 같다  
        ![그래프 신경망](./img/day25/gnn7.png)
* 그래프 신경망을 학습할 때에는 그래프의 모든 정점을 사용하지 않아도 된다
* 학습에 사용할 대상 정점을 뽑아 학습 데이터를 구성한다
* 선택한 대상 정점들을 갖고 model을 학습한 후, 모든 정점에 대한 embedding vector들을 inference 하면 된다  
        ![그래프 신경망](./img/day25/gnn8.png)  
        ![그래프 신경망](./img/day25/gnn9.png)
* 학습된 model만 있으면, 학습 이후에 추가된 정점의 embedding도 얻을 수 있다
* 학습된 그래프 신경망을 새로운 그래프에 적용할 수도 있다
  * (다만 이 경우엔 두 그래프의 정점들의 속성이 동일해야 할 것으로 보임)

### 그래프 신경망 변형

* 위에서 살펴본 집계함수 외에도 다양한 형태의 집계함수를 사용할 수 있다
* 지금부터 나오는 변형된 그래프 신경망들은 위의 기본적인 그래프 신경망에서 집계함수를 바꾼 것임

#### 그래프 합성곱 신경망 (Graph Convolutional Network, GCN)

* 이름에 convolutional이 들어가 있지만, CNN과 밀접한 관련이 있는건 아님. GNN의 일부임.
* 그래프 합성곱 신경망의 집계 함수를 기본적인 GNN의 집계함수와 비교해 보면 다음과 같다  
        ![그래프 신경망](./img/day25/gnn10.png)

#### GraphSAGE

* GraphSAGE의 집계함수는 아래와 같다
* 이웃들의 embedding을 aggregation 함수를 이용해 합친 후, 이전 layer에서의 자신의 embedding과 concatenation 한다    
        ![그래프 신경망](./img/day25/gnn11.png)  
* aggregation 함수로는 평균, 풀링, LSTM 등이 사용될 수 있다
        ![그래프 신경망](./img/day25/gnn12.png)

### 합성곱 신경망(CNN)과의 비교

* CNN과 GNN의 유사성
  * CNN과 GNN은 모두 이웃의 정보를 집계하는 과정을 반복함 (CNN에선 filter를 이용)

* *CNN과 GNN의 차이
  * CNN에서는 이웃의 수가 균일하지만 GNN에서는 아님
  * 그래프의 인접 행렬에서 인접한 행과 열이 이웃을 의미하지 않음

* 그래프의 인접 행렬에 합성곱 신경망을 적용하면 절대 안된다
* 그래프에는 CNN이 아닌 GNN을 적용해야 함 -> 많은 사람들이 실수하는 부분

### 실습 - DGL 라이브러리와 GraphSAGE를 이용한 정점 분류

// TODO

## 그래프 신경망이란 무엇일까? (심화)