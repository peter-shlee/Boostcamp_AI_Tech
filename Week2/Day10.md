# Day 10 - 시각화 / 통계학

## 시각화

* python의 데이터 시각화 도구 matplotlib와 seaborn 사용법을 배움
* 자세한 학습정리는 [여기](./Graph/Graph.md)

## 통계학

* 통계적 모델링은 적절한 가정 위에서 확률분포를 추정하는 것이 목표이다
* 하지만 일부의 데이터를 관찰해서 모집단의 분포를 정확히 알아내는 것은 불가능함
* 그래서 모집단의 특징을 파악하는데 포본 평균, 표본 분산등을 이용함
* 표본의 결과를 이용하는 방법은 근사적으로 확률분포를 추정하는 수밖에 없다
* 여기에는 두가지 방법론이 있음
  * 모수적(parametric) 방법론은 데이터가 특정 확률분포를 따른다고 선험적으로 가정한 후 그 분포를 결정하는 모수(parameter)를 추정
  * 비모수(nonparametric) 방법론은 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀜
    * 기계학습의 많은 방법론은 이 비모수 방법론에 속함

* 히스토그램을 통해 모양을 관찰한 뒤 확률분포를 가정함. 이때 데이터를 생성하는 원리를 고려해야 함
  * 데이터가 2개의 값(0 or 1)만 가지는 경우 -> 베르누이 분포
  * 데이터가 n개의 이산적인 값을 가지는 경우 -> 카테고리 분포
  * 데이터가 [0, 1] 사이에서 값을 가지는 경우 -> 베타 분포
  * 데이터가 0 이상의 값을 가지는 경우 -> 감마 분포, 로그 정규 분포 등
  * 데이터가 R 전체에서 값을 가지는 경우 -> 정규분포, 라플라스 분포 등

* 통계량의 확률분포를 표집분포라고 한다. 표본 평균의 표집분포(sampling distribution)는 N이 커질수록 정규분포를 따르게 된다
  * 이를 중심 극한 정리라고 부른다.
  * 모집단의 분포가 정규분포를 따르지 않아도 성립한다.
  * 표집분포와 표본분포는 다르다. 표집분포는 표본의 통계량 (표본 평균, 표본 분산)의 확률분포임

* 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 최대가능도 추정법(MLE, maximum, likelihood estimation) 이다
  ![최대가능도](./img/최대가능도.png)
* 가능도를 스케일해도 결과는 동일하기 때문에 로그 가능도를 최적화한다
  * 로그 가능도를 사용할 경우 곱셈 연산이 덧셈 연산으로 바뀌기 때문에 계산 복잡도가 줄어든다
* 최대 로그 가능도를 추정하는 것은 크로스 엔트로피(두 분포의 차이)를 최소화하는 것과 동치이다
  ![로그가능도](./img/로그가능도.png)

* 정규분포의 최대 가능도 추정 방법
  ![최대가능도-정규분포1](./img/최대가능도-정규분포1.png)
  두 미분이 모두 0이 되는 $\mu, \sigma$를 찾으면 가능도를 최대화하게 됨  
  ![최대가능도-정규분포2](./img/최대가능도-정규분포2.png)

* 카테고리 분포의 최대 가능도 추정 방법
  ![최대가능도-카테코리분포1](./img/최대가능도-카테고리분포1.png)
  ![최대가능도-카테코리분포2](./img/최대가능도-카테고리분포2.png)
  ![최대가능도-카테코리분포3](./img/최대가능도-카테고리분포3.png)

* 딥러닝에서 최대 가능도 추정 방법
  ![최대가능도-딥러닝](./img/최대가능도-딥러닝.png)

* 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와, 데이터에서관찰되는 확률분포의 거리를 통해 유도됨
* 두 확률분포 사이의 거리는 다음과 같은 함수들을 이용함
  * 총 변동 거리
  * 쿨백-라이블러 발산
  * 바슈타인 거리

* 최대 가능도 추정법은 쿨백-라이블러 발산을 최소화 하는 것과 같음

다음은 cross entropy에 대해 정말 잘 정리된 글들이다. 참고해보자  
https://hyunw.kim/blog/2017/10/14/Entropy.html  
https://hyunw.kim/blog/2017/10/26/Cross_Entropy.html  
https://hyunw.kim/blog/2017/10/27/KL_divergence.html  
https://hoya012.github.io/blog/cross_entropy_vs_kl_divergence/

다음은 김태희 조교님이 25조 채널에 남겨주신 글을 요약한 것

* 일반적으로 분류문제의 loss는 softmax output에 negative log 를 씌워 cross entropy loss라는 이름으로 사용하게 됩니다
* softmax output에 negative log를 씌우면 softmax가 확률이기 때문에 likelihood라 볼 수 있습니다 그래서 cross entropy loss를 negative log likelihood라 부르기도 합니다
* 그럼 cross entropy라는건 무엇이냐? rough하게는 두 확률 분포의 차이를 나타내기 위해 사용합니다. 보통 우리가 모델로 가까워지고자 하는 확률 분포는 training set의 분포이기 때문에 이 확률 분포를 기준으로 모델이 해당 확률 분포를 근사하기 위해 열심히 optimization을 한다고 생각해주시면 됩니다
* 따라서, cross entropy 공식에 따라 분류 모델의 loss를 minimize하도록 식을 세우면 softmax output에 negative log 씌운 결과값을 minimize하는 것과 완전히 동일하게 됩니다
* 또 cross entropy는 entropy + KL divergence로 이루어져 있는데요 cross entropy를 minimize하는 것은  KL divergence를 minimize하는 것과 같습니다. KL term을 제외한 부분(entropy)이 고정된 상수값이어서, minimize하는 관점에서는 KL divergence를 minimize하는 것과 같게 됩니다