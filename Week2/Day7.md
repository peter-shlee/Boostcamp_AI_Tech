# Day 7 - 경사하강법

- [Day 7 - 경사하강법](#day-7---경사하강법)
  - [미분](#미분)
    - [미분이란?](#미분이란)
    - [미분을 어디에 사용할까?](#미분을-어디에-사용할까)
    - [변수가 벡터인 경우](#변수가-벡터인-경우)
  - [선형회귀분석](#선형회귀분석)
    - [경사하강법을 이용한 선형회귀분석](#경사하강법을-이용한-선형회귀분석)
    - [확률적 경사하강법](#확률적-경사하강법)
  - [경사하강법 Quiz](#경사하강법-quiz)
  - [TODO](#todo)

## 미분

### 미분이란?

* 미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구
* 최적화에서 제일 많이 사용하는 기법임
* $f' (x) = \lim_{h \to 0}{f(x + h) - f(x)\over h}$
* 미분을 손으로 계산하려면 일일이 $h \to 0$ 극한을 계산해야 한다
* 최근엔 컴퓨터를 이용해 미분을 계산한다

    ```Python
    import sympy as sym
    from sympy.abc import x

    sym.diff(sym.poly(x**2 + 2*x + 3), x)
    ```

### 미분을 어디에 사용할까?

* 미분은 주어진 점 $(x, f(x))$ 에서의 접선의 기울기를 구한다
* 한 점에서 접선의 기울기를 알면 어느 방향으로 점을움직여야 함수값이 증가/감소 하는지 알 수 있다
* 값을 증가시키고 싶다면 미분값을 더한다 (경사상승법)
* 값을 감소시키고 싶다면 미분값을 뺀다 (경사하강법)
* 극값에서는 미분값이 0이므로 움직임을 멈춘다
* 경사하강법을 pseudo 코드로 나타내면 다음과 같다


    ```Python
    var = init # 시작점(init) 지정
    grad = gradient(var) # 시작점에서의 미분값 구함
    while(abs(grad) > eps): # 알고리즘 종료 조건을 만족할 때까지 반복
        var = var - lr * grad # 값이 감소하는 방향으로 이동한다. 여기서 lr은 학습률
        grad = gradient(var) # 이동해간 위치에서 미분값을 구한다
    ```

    컴퓨터로 계산하면 미분값이 정확이 0이 되지 않는다(부동소수점 오차). 따라서 종료조건을 만들어 미분값이 종료조건 값보다 작아지면 종료하도록 한다.  
    미분값에 학습률을 곱해 경사하강법의 속도를 조절한다. 이 값을 적절히 정해줘야 학습이 잘 된다.

### 변수가 벡터인 경우

* 변수가 벡터이면 편미분을 사용한다
* 벡터는 n차원 공간의 한 점이다. n차원 공간에서는 여러 방향으로 움직일 수 있기 때문에 +/-만으로 움직일 방향이 결정되지 않는다.
* 편미분을 이용하면 특정 좌표축 상에서의 변화율을 구할 수 있다
* n차원 벡터인 경우 n번 편미분 하여 기울기(gradient)를 구할 수 있다
* 편미분하여 구한 gradient 벡터를 경사하강법/경사상승법에 사용한다
* 변수가 벡터인 경우의 경사하강법 pseudo 코드는 다음과 같다

    ```Python
    var = init # 시작점(init) 지정
    grad = gradient(var) # 시작점에서의 gradient 벡터를 구함
    while(norm(grad > eps)): # 알고리즘 종료 조건을 만족할 때까지 반복. 벡터는 절대값 대신 노름을 이용한다
        var = var - lr * grad # 값이 감소하는 방향으로 이동한다. 여기서 lr은 학습률
        grad = gradient(var) # 이동해간 위치에서 gradient 벡터를 구한다
    ```

    종료조건을 검사할 때 노름을 사용한다.

## 선형회귀분석

### 경사하강법을 이용한 선형회귀분석

* Day 5에서는 유사역행렬을 이용해 선형회귀분석을 하는 법을 배웠다
* 경사하강법을 이용해 선형회귀분석을 할 수 있다
* 선형모델이 아닌 다른 모델에서도 경사하강법을 이용할 수 있기 때문에 경사하강법이 유사 역행렬을 이용하는 것 보다 더 일반적인 최적화 방법이다
* 경사하강법 기반 선형회귀 알고리즘의 psudo code는 다음과 같다

    ```Python
    for t in range(T): # 종료 조건 : 일정 학습 횟수
        error = y - x @ beta
        grad = -transpose(X) @ error # gradient 벡터를 구함
        beta = beta - lr * grad # beta값 갱신
    ```

    학습률과 학습 횟수를 적절히 조절해야 원하는 결과를 얻을 수 있다

### 확률적 경사하강법

* 이론적으로 경사하강법은 미분 가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습 횟수를 선택했을 때 수렴이 보장된다
* 선형회귀의 경우 목적식이 회귀계수 $\beta$에 대해 볼록함수이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장된다
* 하지만 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되진 않는다
  * local minimum에 갇힐 수 있다. 이 경우엔 목표인 global minimum에 도달하지 못한다
* 확률적 경사하강법(SGD)은 데이터의 일부를 활용해 gradient를 업데이트한다
  * 데이터 한개를 활용 - SGD
  * 데이터 일부를 활용 - mini batch SGD
  * 하지만 보통 mini batch SGD를 그냥 SGD라고 부른다. 이 글에 나오는 SGD 역시 mini batch SGD를 뜻함
* SGD를 사용하면 볼록이 아닌(non-convex) 목적식을 최적화할 수 있다
* 데이터의 일부를 가지고 parameter를 업데이트 하기 때문에 연산자원을 좀 더 효율적으로 활용하는데 도움이 된다
* 확률적으로 미니배치를 선택하므로 목적식의 모양이 계속 바뀌게 된다
  * 이 성질 덕분에 극솟점에서 탈출이 가능하다
* 볼록이 아닌 목적식에서도 사용 가능하므로 경사하강법보다 머신러닝 학습에 더 효율적이다
* 학습률, 학습 횟수에 더해 미니배치 사이즈도 고려해야한다

## 경사하강법 Quiz

1. 편미분하기 
   * 편미분해서 풀었다
2. 주어진 점 (x, f(x))에서의 접선의 기울기가 음수라면, x를 증가시키면 함수값 f(x)가 증가하는가?
   * 함수값을 증가하는 방향으로 이동하려면 x에 미분값을 더해야 한다
   * 이 문제의 경우 x에 음수인 미분값을 더하면 함수값이 증가한다
   * 이 말은 x를 감소시켜야 함수값이 증가한다는 뜻이다
   * 따라서 답은 아니오
3. 극소값의 위치를 구할 때 사용하는 방법은?
   * 경사하강법
4. 경사하강법을 사용하여 구한 구현 함수의 극소값으 위치는 해당 함수의 최소값의 위치가 아닐 수 있는가?
   * global minimum이 아닌 local minimum일 수 있다
   * 따라서 답은 예
5. f(x, y, z)의 gradient 벡터 구하는 문제
   * 편미분 해서 풀었다

## TODO

* 어제, 오늘 수업내용 복습
* 딥러닝 관련 논문 찾아 읽어보기
* 수업자료 파이썬 코드 이해하기