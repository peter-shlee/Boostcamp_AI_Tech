# 1. Spacyë¥¼ ì´ìš©í•œ ì˜ì–´ ì „ì²˜ë¦¬


```python
import spacy
spacy_en = spacy.load('en')
```


```python
nlp = spacy.load('en_core_web_sm')
```

### 1.1 Tokenezation


```python
text = nlp('Naver Connect and Upstage Boostcamp')
print ([token.text for token in text])
```

    ['Naver', 'Connect', 'and', 'Upstage', 'Boostcamp']



```python
doc = nlp('This assignment is about Natural Language Processing.' 'In this assignment, we will do preprocessing')
print ([token.text for token in doc])
```

    ['This', 'assignment', 'is', 'about', 'Natural', 'Language', 'Processing', '.', 'In', 'this', 'assignment', ',', 'we', 'will', 'do', 'preprocessing']



```python
text=nlp("The film's development began when Marvel Studios received a loan from Merrill Lynch in April 2005. After the success of the film Iron Man in May 2008, \
Marvel announced that The Avengers would be released in July 2011 and would bring together Tony Stark, Steve Rogers, Bruce Banner, and Thor from Marvel's previous films. \
With the signing of Johansson as Natasha Romanoff in March 2009, the film was pushed back for a 2012 release. Whedon was brought on board in April 2010 and rewrote the original screenplay by Zak Penn. Production began in April 2011 in Albuquerque, \
New Mexico, before moving to Cleveland, Ohio in August and New York City in September. The film has more than 2,200 visual effects shots.")
```

### 1.2 ë¶ˆìš©ì–´ (Stopword)


```python
spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS
for stop_word in list(spacy_stopwords)[:30]:
  print(stop_word)
```

    using
    twenty
    call
    mine
    move
    other
    well
    where
    again
    further
    â€™s
    between
    my
    on
    ourselves
    whereas
    therein
    third
    anyway
    've
    here
    wherever
    may
    alone
    his
    yourselves
    did
    even
    n't
    which



```python
stopword_text = [token for token in text if not token.is_stop]
print(stopword_text)
```

    [film, development, began, Marvel, Studios, received, loan, Merrill, Lynch, April, 2005, ., success, film, Iron, Man, 2008, ,, Marvel, announced, Avengers, released, July, 2011, bring, Tony, Stark, ,, Steve, Rogers, ,, Bruce, Banner, ,, Thor, Marvel, previous, films, ., signing, Johansson, Natasha, Romanoff, March, 2009, ,, film, pushed, 2012, release, ., Whedon, brought, board, April, 2010, rewrote, original, screenplay, Zak, Penn, ., Production, began, April, 2011, Albuquerque, ,, New, Mexico, ,, moving, Cleveland, ,, Ohio, August, New, York, City, September, ., film, 2,200, visual, effects, shots, .]


### 1.3 Lemmatization 


```python
for token in text[:20]:
  print (token, "-", token.lemma_) # ì‚¬ì „í˜• ë‹¨ì–´
```

    The - the
    film - film
    's - 's
    development - development
    began - begin
    when - when
    Marvel - Marvel
    Studios - Studios
    received - receive
    a - a
    loan - loan
    from - from
    Merrill - Merrill
    Lynch - Lynch
    in - in
    April - April
    2005 - 2005
    . - .
    After - after
    the - the


### 1.4 ê·¸ì™¸ token classì˜ attributes 

https://spacy.io/api/token#attributes


```python
print("token \t is_punct \t is_space \t shape_ \t is_stop")
print("="*70)
for token in text[21:31]:
  print(token,"\t", token.is_punct, "\t\t",token.is_space,"\t\t", token.shape_, "\t\t",token.is_stop)
```

    token 	 is_punct 	 is_space 	 shape_ 	 is_stop
    ======================================================================
    of 	 False 		 False 		 xx 		 True
    the 	 False 		 False 		 xxx 		 True
    film 	 False 		 False 		 xxxx 		 False
    Iron 	 False 		 False 		 Xxxx 		 False
    Man 	 False 		 False 		 Xxx 		 False
    in 	 False 		 False 		 xx 		 True
    May 	 False 		 False 		 Xxx 		 True
    2008 	 False 		 False 		 dddd 		 False
    , 	 True 		 False 		 , 		 False
    Marvel 	 False 		 False 		 Xxxxx 		 False


## ë¹ˆì¹¸ì™„ì„± ê³¼ì œ1


```python
def is_token_allowed(token):
# stopwordì™€ punctutationì„ ì œê±°í•´ì£¼ì„¸ìš”.

  #ifë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
  
  ##TODO#
  if token.is_stop or token.is_punct:
  ##TODO##
    return False
  return True

def preprocess_token(token):
  #lemmatizationì„ ì‹¤í–‰í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. 
  return token.lemma_.strip().lower()

filtered_tokens = [preprocess_token(token) for token in text if is_token_allowed(token)]
answer=['film', 'development','begin', 'marvel','studios', 'receive','loan', 'merrill','lynch', 'april','2005', 'success','film', 'iron','man', '2008','marvel','announce', 'avengers','release', 'july','2011', 'bring','tony', 'stark','steve', 'rogers','bruce', 'banner','thor', 'marvel','previous', 'film','signing', 'johansson','natasha','romanoff','march','2009','film','push','2012','release','whedon','bring','board','april','2010','rewrote','original','screenplay','zak','penn','production','begin','april','2011','albuquerque','new','mexico','move','cleveland','ohio','august','new','york','city','september','film','2,200','visual','effect','shot']
assert filtered_tokens == answer
```

    ['film', 'development', 'begin', 'marvel', 'studios', 'receive', 'loan', 'merrill', 'lynch', 'april', '2005', 'success', 'film', 'iron', 'man', '2008', 'marvel', 'announce', 'avengers', 'release', 'july', '2011', 'bring', 'tony', 'stark', 'steve', 'rogers', 'bruce', 'banner', 'thor', 'marvel', 'previous', 'film', 'signing', 'johansson', 'natasha', 'romanoff', 'march', '2009', 'film', 'push', '2012', 'release', 'whedon', 'bring', 'board', 'april', '2010', 'rewrote', 'original', 'screenplay', 'zak', 'penn', 'production', 'begin', 'april', '2011', 'albuquerque', 'new', 'mexico', 'move', 'cleveland', 'ohio', 'august', 'new', 'york', 'city', 'september', 'film', '2,200', 'visual', 'effect', 'shot']



```python

```

# 2. í•œêµ­ì–´ ì „ì²˜ë¦¬

### 2.1 Mecabë¥¼ ì´ìš©í•œ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì§•


```python
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
%cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh
```

    Cloning into 'Mecab-ko-for-Google-Colab'...
    remote: Enumerating objects: 91, done.[K
    remote: Counting objects: 100% (91/91), done.[K
    remote: Compressing objects: 100% (85/85), done.[K
    remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0[K
    Unpacking objects: 100% (91/91), done.
    /content/Mecab-ko-for-Google-Colab
    Installing konlpy.....
    Collecting konlpy
    [?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.4MB 1.2MB/s 
    [?25hCollecting tweepy>=3.7.0
      Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl
    Collecting beautifulsoup4==4.6.0
    [?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 11.0MB/s 
    [?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)
    Collecting JPype1>=0.7.0
    [?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460kB 54.9MB/s 
    [?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)
    Collecting colorama
      Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl
    Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)
    Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)
    Requirement already satisfied: typing-extensions; python_version < "3.8" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)
    Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == "socks" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)
    Installing collected packages: tweepy, beautifulsoup4, JPype1, colorama, konlpy
      Found existing installation: tweepy 3.6.0
        Uninstalling tweepy-3.6.0:
          Successfully uninstalled tweepy-3.6.0
      Found existing installation: beautifulsoup4 4.6.3
        Uninstalling beautifulsoup4-4.6.3:
          Successfully uninstalled beautifulsoup4-4.6.3
    Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0
    Done
    Installing mecab-0.996-ko-0.9.2.tar.gz.....
    Downloading mecab-0.996-ko-0.9.2.tar.gz.......
    from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
    --2021-02-15 14:49:41--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
    Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22e9:9f55, 2406:da00:ff00::3403:4be7, ...
    Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=aFHEfc3q4rkfwwznbyZZsvSmHG4%3D&Expires=1613402306&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]
    --2021-02-15 14:49:42--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=aFHEfc3q4rkfwwznbyZZsvSmHG4%3D&Expires=1613402306&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None
    Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.17.108
    Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.17.108|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 1414979 (1.3M) [application/x-tar]
    Saving to: â€˜mecab-0.996-ko-0.9.2.tar.gzâ€™
    
    mecab-0.996-ko-0.9. 100%[===================>]   1.35M  2.64MB/s    in 0.5s    
    
    2021-02-15 14:49:43 (2.64 MB/s) - â€˜mecab-0.996-ko-0.9.2.tar.gzâ€™ saved [1414979/1414979]
    
    Done
    Unpacking mecab-0.996-ko-0.9.2.tar.gz.......
    Done
    Change Directory to mecab-0.996-ko-0.9.2.......
    installing mecab-0.996-ko-0.9.2.tar.gz........
    configure
    make
    make check
    make install
    ldconfig
    Done
    Change Directory to /content
    Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......
    from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
    --2021-02-15 14:51:08--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
    Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c3:9b0a, 2406:da00:ff00::22e9:9f55, ...
    Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=O5vnnNJwQU44mR4Z2oATlFZopX4%3D&Expires=1613402395&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]
    --2021-02-15 14:51:08--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=O5vnnNJwQU44mR4Z2oATlFZopX4%3D&Expires=1613402395&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None
    Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.206.115
    Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.206.115|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 49775061 (47M) [application/x-tar]
    Saving to: â€˜mecab-ko-dic-2.1.1-20180720.tar.gzâ€™
    
    mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  24.8MB/s    in 1.9s    
    
    2021-02-15 14:51:10 (24.8 MB/s) - â€˜mecab-ko-dic-2.1.1-20180720.tar.gzâ€™ saved [49775061/49775061]
    
    Done
    Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......
    Done
    Change Directory to mecab-ko-dic-2.1.1-20180720
    Done
    installing........
    configure
    make
    make install
    apt-get update
    apt-get upgrade
    apt install curl
    apt install git
    bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)
    Done
    Successfully Installed
    Now you can use Mecab
    from konlpy.tag import Mecab
    mecab = Mecab()
    ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€ ë°©ë²• : https://bit.ly/3k0ZH53
    NameError: name 'Tagger' is not defined ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŸ°íƒ€ì„ì„ ì¬ì‹¤í–‰ í•´ì£¼ì„¸ìš”
    ë¸”ë¡œê·¸ì— í•´ê²° ë°©ë²•ì„ ë‚¨ê²¨ì£¼ì‹  tanaë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤.



```python
from konlpy.tag import Mecab
import operator
tokenizer = Mecab()
```


```python
text="ìµœê°•ì˜ ìŠˆí¼íˆì–´ë¡œë“¤ì´ ëª¨ì˜€ë‹¤! ì§€êµ¬ì˜ ìš´ëª…ì„ ê±´ ê±°ëŒ€í•œ ì „ìŸì´ ì‹œì‘ëœë‹¤! ì§€êµ¬ì˜ ì•ˆë³´ê°€ ìœ„í˜‘ë‹¹í•˜ëŠ” ìœ„ê¸°ì˜ ìƒí™©ì—ì„œ ìŠˆí¼íˆì–´ë¡œë“¤ì„ ë¶ˆëŸ¬ëª¨ì•„ ì„¸ìƒì„ êµ¬í•˜ëŠ”, ì¼ëª… ì–´ë²¤ì ¸ìŠ¤ ì‘ì „. ì—ë„ˆì§€ì› í…Œì„œë™íŠ¸ë¥¼ ì´ìš©í•œ ì ì˜ ë“±ì¥ìœ¼ë¡œ ì¸ë¥˜ê°€ ìœ„í—˜ì— ì²˜í•˜ì êµ­ì œí‰í™”ìœ ì§€ê¸°êµ¬ì¸ ì‰´ë“œì˜ êµ­ì¥ ë‹‰ í“¨ë¦¬ëŠ” ì–´ë²¤ì ¸ìŠ¤ ì‘ì „ì„ ìœ„í•´ ì „ ì„¸ê³„ì— í©ì–´ì ¸ ìˆë˜ ìŠˆí¼íˆì–´ë¡œë“¤ì„ ì°¾ì•„ë‚˜ì„ ë‹¤. ì•„ì´ì–¸ë§¨ë¶€í„° í† ë¥´, í—í¬, ìº¡í‹´ ì•„ë©”ë¦¬ì¹´ëŠ” ë¬¼ë¡ , ì‰´ë“œì˜ ìš”ì›ì¸ ë¸”ë™ ìœ„ë„ìš°, í˜¸í¬ì•„ì´ê¹Œì§€, ìµœê³ ì˜ ìŠˆí¼íˆì–´ë¡œë“¤ì´ ì–´ë²¤ì ¸ìŠ¤ì˜ ë©¤ë²„ë¡œ ëª¨ì´ê²Œ ë˜ì§€ë§Œ, ê°ê¸° ê°œì„±ì´ ê°•í•œ ì´ë“¤ì˜ ë§Œë‚¨ì€ ì˜ˆìƒì¹˜ ëª»í•œ ë°©í–¥ìœ¼ë¡œ í˜ëŸ¬ê°€ëŠ”ë°â€¦ ì§€êµ¬ì˜ ìš´ëª…ì„ ê±´ ê±°ëŒ€í•œ ì „ìŸ ì•ì— ì–´ë²¤ì ¸ìŠ¤ ì‘ì „ì€ ì„±ê³µí•  ìˆ˜ ìˆì„ê¹Œ?"
```


```python
print(tokenizer.morphs(text))
```

    ['ìµœê°•', 'ì˜', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ë“¤', 'ì´', 'ëª¨ì˜€', 'ë‹¤', '!', 'ì§€êµ¬', 'ì˜', 'ìš´ëª…', 'ì„', 'ê±´', 'ê±°ëŒ€', 'í•œ', 'ì „ìŸ', 'ì´', 'ì‹œì‘', 'ëœë‹¤', '!', 'ì§€êµ¬', 'ì˜', 'ì•ˆë³´', 'ê°€', 'ìœ„í˜‘', 'ë‹¹í•˜', 'ëŠ”', 'ìœ„ê¸°', 'ì˜', 'ìƒí™©', 'ì—ì„œ', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ë“¤', 'ì„', 'ë¶ˆëŸ¬', 'ëª¨ì•„', 'ì„¸ìƒ', 'ì„', 'êµ¬í•˜', 'ëŠ”', ',', 'ì¼', 'ëª…', 'ì–´ë²¤ì ¸ìŠ¤', 'ì‘ì „', '.', 'ì—ë„ˆì§€ì›', 'í…Œ', 'ì„œ', 'ë™', 'íŠ¸', 'ë¥¼', 'ì´ìš©', 'í•œ', 'ì ', 'ì˜', 'ë“±ì¥', 'ìœ¼ë¡œ', 'ì¸ë¥˜', 'ê°€', 'ìœ„í—˜', 'ì—', 'ì²˜í•˜', 'ì', 'êµ­ì œ', 'í‰í™”', 'ìœ ì§€', 'ê¸°êµ¬', 'ì¸', 'ì‰´ë“œ', 'ì˜', 'êµ­ì¥', 'ë‹‰', 'í“¨ë¦¬', 'ëŠ”', 'ì–´ë²¤ì ¸ìŠ¤', 'ì‘ì „', 'ì„', 'ìœ„í•´', 'ì „', 'ì„¸ê³„', 'ì—', 'í©ì–´ì ¸', 'ìˆ', 'ë˜', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ë“¤', 'ì„', 'ì°¾', 'ì•„', 'ë‚˜ì„ ë‹¤', '.', 'ì•„ì´ì–¸ë§¨', 'ë¶€í„°', 'í† ë¥´', ',', 'í—í¬', ',', 'ìº¡í‹´', 'ì•„ë©”ë¦¬ì¹´', 'ëŠ”', 'ë¬¼ë¡ ', ',', 'ì‰´ë“œ', 'ì˜', 'ìš”ì›', 'ì¸', 'ë¸”ë™', 'ìœ„', 'ë„ìš°', ',', 'í˜¸í¬ì•„ì´', 'ê¹Œì§€', ',', 'ìµœê³ ', 'ì˜', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ë“¤', 'ì´', 'ì–´ë²¤ì ¸ìŠ¤', 'ì˜', 'ë©¤ë²„', 'ë¡œ', 'ëª¨ì´', 'ê²Œ', 'ë˜', 'ì§€ë§Œ', ',', 'ê°ê¸°', 'ê°œì„±', 'ì´', 'ê°•í•œ', 'ì´', 'ë“¤', 'ì˜', 'ë§Œë‚¨', 'ì€', 'ì˜ˆìƒ', 'ì¹˜', 'ëª»í•œ', 'ë°©í–¥', 'ìœ¼ë¡œ', 'í˜ëŸ¬ê°€', 'ëŠ”ë°', 'â€¦', 'ì§€êµ¬', 'ì˜', 'ìš´ëª…', 'ì„', 'ê±´', 'ê±°ëŒ€', 'í•œ', 'ì „ìŸ', 'ì•', 'ì—', 'ì–´ë²¤ì ¸ìŠ¤', 'ì‘ì „', 'ì€', 'ì„±ê³µ', 'í• ', 'ìˆ˜', 'ìˆ', 'ì„ê¹Œ', '?']



```python
stopwords=['ì˜','ê°€','ì´','ì€','ë‹¤','ë“¤','ì„','ëŠ”','ì¸','ìœ„í•´','ê³¼','ë˜','ë„','ë¥¼','ë¡œ','ê²Œ','ìœ¼ë¡œ','ê¹Œì§€','ì','ì—','ì„ê¹Œ','ëŠ”ë°','ì¹˜','ì™€','í•œ','í•˜ë‹¤']
```


```python
tokenized_text = [word for word in tokenizer.morphs(text) if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
print(tokenized_text)
```

    ['ìµœê°•', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ëª¨ì˜€', '!', 'ì§€êµ¬', 'ìš´ëª…', 'ê±´', 'ê±°ëŒ€', 'ì „ìŸ', 'ì‹œì‘', 'ëœë‹¤', '!', 'ì§€êµ¬', 'ì•ˆë³´', 'ìœ„í˜‘', 'ë‹¹í•˜', 'ìœ„ê¸°', 'ìƒí™©', 'ì—ì„œ', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ë¶ˆëŸ¬', 'ëª¨ì•„', 'ì„¸ìƒ', 'êµ¬í•˜', ',', 'ì¼', 'ëª…', 'ì–´ë²¤ì ¸ìŠ¤', 'ì‘ì „', '.', 'ì—ë„ˆì§€ì›', 'í…Œ', 'ì„œ', 'ë™', 'íŠ¸', 'ì´ìš©', 'ì ', 'ë“±ì¥', 'ì¸ë¥˜', 'ìœ„í—˜', 'ì²˜í•˜', 'êµ­ì œ', 'í‰í™”', 'ìœ ì§€', 'ê¸°êµ¬', 'ì‰´ë“œ', 'êµ­ì¥', 'ë‹‰', 'í“¨ë¦¬', 'ì–´ë²¤ì ¸ìŠ¤', 'ì‘ì „', 'ì „', 'ì„¸ê³„', 'í©ì–´ì ¸', 'ìˆ', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ì°¾', 'ì•„', 'ë‚˜ì„ ë‹¤', '.', 'ì•„ì´ì–¸ë§¨', 'ë¶€í„°', 'í† ë¥´', ',', 'í—í¬', ',', 'ìº¡í‹´', 'ì•„ë©”ë¦¬ì¹´', 'ë¬¼ë¡ ', ',', 'ì‰´ë“œ', 'ìš”ì›', 'ë¸”ë™', 'ìœ„', 'ë„ìš°', ',', 'í˜¸í¬ì•„ì´', ',', 'ìµœê³ ', 'ìŠˆí¼', 'íˆì–´ë¡œ', 'ì–´ë²¤ì ¸ìŠ¤', 'ë©¤ë²„', 'ëª¨ì´', 'ë˜', 'ì§€ë§Œ', ',', 'ê°ê¸°', 'ê°œì„±', 'ê°•í•œ', 'ë§Œë‚¨', 'ì˜ˆìƒ', 'ëª»í•œ', 'ë°©í–¥', 'í˜ëŸ¬ê°€', 'â€¦', 'ì§€êµ¬', 'ìš´ëª…', 'ê±´', 'ê±°ëŒ€', 'ì „ìŸ', 'ì•', 'ì–´ë²¤ì ¸ìŠ¤', 'ì‘ì „', 'ì„±ê³µ', 'í• ', 'ìˆ˜', 'ìˆ', '?']


### 2.2 ìŒì ˆ ë‹¨ìœ„ í† í¬ë‚˜ì´ì§• ì‹¤ìŠµ


```python
starry_night=['ê³„ì ˆì´ ì§€ë‚˜ê°€ëŠ” í•˜ëŠ˜ì—ëŠ”',
'ê°€ì„ë¡œ ê°€ë“ ì°¨ ìˆìŠµë‹ˆë‹¤.',
'ë‚˜ëŠ” ì•„ë¬´ ê±±ì •ë„ ì—†ì´',
'ê°€ì„ ì†ì˜ ë³„ë“¤ì„ ë‹¤ í—¬ ë“¯í•©ë‹ˆë‹¤.',
'ê°€ìŠ´ ì†ì— í•˜ë‚˜ ë‘˜ ìƒˆê²¨ì§€ëŠ” ë³„ì„',
'ì´ì œ ë‹¤ ëª» í—¤ëŠ” ê²ƒì€',
'ì‰¬ì´ ì•„ì¹¨ì´ ì˜¤ëŠ” ê¹Œë‹­ì´ìš”,',
'ë‚´ì¼ ë°¤ì´ ë‚¨ì€ ê¹Œë‹­ì´ìš”,',
'ì•„ì§ ë‚˜ì˜ ì²­ì¶˜ì´ ë‹¤í•˜ì§€ ì•Šì€ ê¹Œë‹­ì…ë‹ˆë‹¤.',
'ë³„ í•˜ë‚˜ì— ì¶”ì–µê³¼',
'ë³„ í•˜ë‚˜ì— ì‚¬ë‘ê³¼',
'ë³„ í•˜ë‚˜ì— ì“¸ì“¸í•¨ê³¼',
'ë³„ í•˜ë‚˜ì— ë™ê²½ê³¼',
'ë³„ í•˜ë‚˜ì— ì‹œì™€',
'ë³„ í•˜ë‚˜ì— ì–´ë¨¸ë‹ˆ, ì–´ë¨¸ë‹ˆ,',
"ì–´ë¨¸ë‹˜, ë‚˜ëŠ” ë³„ í•˜ë‚˜ì— ì•„ë¦„ë‹¤ìš´ ë§ í•œë§ˆë””ì”© ë¶ˆëŸ¬ ë´…ë‹ˆë‹¤. ì†Œí•™êµ ë•Œ ì±…ìƒì„ ê°™ì´ í–ˆë˜ ì•„ì´ë“¤ì˜ ì´ë¦„ê³¼, íŒ¨, ê²½, ì˜¥, ì´ëŸ° ì´êµ­ ì†Œë…€ë“¤ì˜ ì´ë¦„ê³¼, ë²Œì¨ ì•„ê¸° ì–´ë¨¸ë‹ˆ ëœ ê³„ì§‘ì• ë“¤ì˜ ì´ë¦„ê³¼, ê°€ë‚œí•œ ì´ì›ƒ ì‚¬ëŒë“¤ì˜ ì´ë¦„ê³¼, ë¹„ë‘˜ê¸°, ê°•ì•„ì§€, í† ë¼, ë…¸ìƒˆ, ë…¸ë£¨, 'í”„ë‘ì‹œìŠ¤ ì ', 'ë¼ì´ë„ˆ ë§ˆë¦¬ì•„ ë¦´ì¼€â€™ ì´ëŸ° ì‹œì¸ì˜ ì´ë¦„ì„ ë¶ˆëŸ¬ ë´…ë‹ˆë‹¤.",
'ì´ë„¤ë“¤ì€ ë„ˆë¬´ë‚˜ ë©€ë¦¬ ìˆìŠµë‹ˆë‹¤.',
'ë³„ì´ ì•„ìŠ¤ë¼ì´ ë©€ë“¯ì´.',
'ì–´ë¨¸ë‹˜,',
'ê·¸ë¦¬ê³  ë‹¹ì‹ ì€ ë©€ë¦¬ ë¶ê°„ë„ì— ê³„ì‹­ë‹ˆë‹¤.',
'ë‚˜ëŠ” ë¬´ì—‡ì¸ì§€ ê·¸ë¦¬ì›Œ',
'ì´ ë§ì€ ë³„ë¹›ì´ ë‚´ë¦° ì–¸ë• ìœ„ì—',
'ë‚´ ì´ë¦„ìë¥¼ ì¨ ë³´ê³ ',
'í™ìœ¼ë¡œ ë®ì–´ ë²„ë¦¬ì—ˆìŠµë‹ˆë‹¤.',
'ë”´ì€ ë°¤ì„ ìƒˆì›Œ ìš°ëŠ” ë²Œë ˆëŠ”',
'ë¶€ë„ëŸ¬ìš´ ì´ë¦„ì„ ìŠ¬í¼í•˜ëŠ” ê¹Œë‹­ì…ë‹ˆë‹¤.',
'ê·¸ëŸ¬ë‚˜ ê²¨ìš¸ì´ ì§€ë‚˜ê³  ë‚˜ì˜ ë³„ì—ë„ ë´„ì´ ì˜¤ë©´',
'ë¬´ë¤ ìœ„ì— íŒŒë€ ì”ë””ê°€ í”¼ì–´ë‚˜ë“¯ì´',
'ë‚´ ì´ë¦„ì ë¬»íŒ ì–¸ë• ìœ„ì—ë„',
'ìë‘ì²˜ëŸ¼ í’€ì´ ë¬´ì„±í•  ê±°ì™¸ë‹¤.',]
```


```python
tokens=[]
for sentence in starry_night:
    tokenezied_text = [token for token in sentence] 
    tokens.extend(tokenezied_text)
```

## ë¹ˆì¹¸ì™„ì„± ê³¼ì œ 2


```python
vocab_dict={}
for token in tokens:
  ##TODO##
  '''
  vocab_dictì— tokenì„ keyë¡œ ë¹ˆë„ìˆ˜ë¥¼ valueë¡œ ì±„ì›Œë„£ìœ¼ì„¸ìš”.
  ì˜ˆì‹œ) vocab_dict={"ë‚˜":3,"ê·¸":5,"ì–´":3,...}
  '''

  if token not in vocab_dict.keys() :
    vocab_dict[token] = 1
  else :
    vocab_dict[token] = vocab_dict[token] + 1


  ##TODO##
```


```python
sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1),reverse=True)
```


```python
vocab=[]
for token,freq in sorted_vocab:
  ##TODO##
  '''
  ì •ë ¬ëœ sorted_vocabì—ì„œ ë¹ˆë„ìˆ˜ê°€ 2 ì´ìƒì¸ tokenì„ vocabì— appendí•˜ì—¬ vocabì„ ì™„ì„±ì‹œí‚¤ì„¸ìš”.
  '''

  if freq >= 2:
    vocab.append(token)

  ##TODO##
  
```


```python
answer=[' ','ì´',',','ë‚˜','ì—','ë‹¤','ë‹ˆ','ë³„','ëŠ”', 'í•˜', '.', 'ì•„', 'ë¦„', 'ì„', 'ì˜', 'ê³¼', 'ê°€', 'ì€', 'ì–´', 'ì§€', 'ë“¤', 'ë¦¬', 'ë¬´', 'ë¨¸', 'ë„', 'ê¹Œ', 'ë‹­', 'ë‚´', 'ëŸ¬', 'ê³„', 'ìŠµ', 'ë“¯', 'ìƒˆ', 'ë‘', 'ì‹œ', "'", 'ë©€', 'ê·¸', 'ê³ ', 'ìœ„', 'ì', 'ë¡œ', 'ìˆ', 'ì†', 'ë‘˜', 'ê²¨', 'ì˜¤', 'ìš”', 'ë°¤', 'ì…', 'ì‚¬', 'ì“¸', 'ê²½', 'ë‹˜', 'ìš´', 'í•œ', 'ë§ˆ', 'ë””', 'ë¶ˆ', 'ë´…', 'ì†Œ', 'ëŸ°', 'ë²Œ', 'ì¨', 'ê¸°', 'ë…¸', 'ìŠ¤', 'ë¼', 'ë„ˆ', 'ì¸', 'ì›Œ', 'ì–¸', 'ë•']
```


```python
assert vocab==answer
```


```python

```
