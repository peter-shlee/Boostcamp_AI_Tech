##**7. Multi-head Attention**
1. Multi-head attention 및 self-attention 구현.
2. 각 과정에서 일어나는 연산과 input/output 형태 이해.

### **필요 패키지 import**


```python
from torch import nn
from torch.nn import functional as F
from tqdm import tqdm

import torch
import math
```

### **데이터 전처리**


```python
pad_id = 0
vocab_size = 100

data = [
  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],
  [60, 96, 51, 32, 90],
  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],
  [75, 51],
  [66, 88, 98, 47],
  [21, 39, 10, 64, 21],
  [98],
  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],
  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],
  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]
]
```


```python
def padding(data):
  max_len = len(max(data, key=len))
  print(f"Maximum sequence length: {max_len}")

  for i, seq in enumerate(tqdm(data)):
    if len(seq) < max_len:
      data[i] = seq + [pad_id] * (max_len - len(seq))

  return data, max_len
```


```python
data, max_len = padding(data)
```

    100%|██████████| 10/10 [00:00<00:00, 22028.91it/s]

    Maximum sequence length: 20


    



```python
data
```




    [[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],
     [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
     [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],
     [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
     [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
     [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
     [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
     [77,
      65,
      51,
      77,
      19,
      15,
      35,
      19,
      23,
      97,
      50,
      46,
      53,
      42,
      45,
      91,
      66,
      3,
      43,
      10],
     [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],
     [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]



### **Hyperparameter 세팅 및 embedding**


```python
d_model = 512  # model의 hidden size
num_heads = 8  # head의 개수
```


```python
embedding = nn.Embedding(vocab_size, d_model)

# B: batch size, L: maximum sequence length
batch = torch.LongTensor(data)  # (B, L)
batch_emb = embedding(batch)  # (B, L, d_model)
```


```python
print(batch_emb)
print(batch_emb.shape)
```

    tensor([[[ 1.8122, -0.5474, -0.8947,  ...,  1.1661,  0.3974, -0.6092],
             [-0.1538,  0.2389, -0.9412,  ...,  1.0004,  1.5163,  0.9858],
             [ 0.0213,  0.0627, -0.3646,  ...,  0.2379,  0.2671, -1.0611],
             ...,
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218]],
    
            [[-1.2344, -0.0087, -2.2043,  ...,  0.0508, -0.7686, -1.0170],
             [-0.4049, -0.8632, -0.5340,  ...,  1.3398,  0.2766,  0.6174],
             [-1.1519,  0.8853,  0.6812,  ...,  0.5298, -0.6986, -1.5910],
             ...,
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218]],
    
            [[ 1.1612,  0.7718, -0.5570,  ..., -1.1367, -0.3501,  0.1102],
             [-1.8450, -1.5175, -0.7429,  ..., -0.0519,  0.1197, -0.5442],
             [-0.1123,  0.5122, -2.3478,  ..., -1.1354,  1.5542,  1.8818],
             ...,
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218]],
    
            ...,
    
            [[-0.4849, -1.8689,  0.5835,  ..., -0.6218, -0.4977, -1.6291],
             [-2.3974,  0.0902,  1.1245,  ...,  0.4491,  0.2115, -0.7222],
             [-1.1519,  0.8853,  0.6812,  ...,  0.5298, -0.6986, -1.5910],
             ...,
             [ 1.4921, -0.7306,  0.5913,  ..., -0.7482, -1.6376, -0.1893],
             [ 1.0533, -1.0812, -0.2678,  ..., -0.9695, -1.1578,  0.8945],
             [ 0.9210,  0.6644,  1.6527,  ..., -1.1735, -0.4490, -0.8119]],
    
            [[-0.8087, -0.4888, -0.0196,  ..., -0.2794,  0.0828, -0.7664],
             [-1.0598, -1.2210, -1.4085,  ...,  1.3440, -0.5048, -0.4194],
             [-0.2543, -0.7057, -0.0516,  ..., -0.3803,  1.2525,  1.2895],
             ...,
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218]],
    
            [[ 0.8696,  0.9360, -0.1594,  ..., -1.5232,  0.8602, -0.2624],
             [-1.0598, -1.2210, -1.4085,  ...,  1.3440, -0.5048, -0.4194],
             [-0.1309, -0.6780, -0.2759,  ...,  0.7445, -0.4458, -1.6422],
             ...,
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218],
             [ 0.3180,  0.1210, -0.7239,  ...,  0.6852,  0.2320, -0.2218]]],
           grad_fn=<EmbeddingBackward>)
    torch.Size([10, 20, 512])


### **Linear transformation & 여러 head로 나누기**

Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다.


```python
w_q = nn.Linear(d_model, d_model)
w_k = nn.Linear(d_model, d_model)
w_v = nn.Linear(d_model, d_model)
```


```python
w_0 = nn.Linear(d_model, d_model)
```


```python
q = w_q(batch_emb)  # (B, L, d_model)
k = w_k(batch_emb)  # (B, L, d_model)
v = w_v(batch_emb)  # (B, L, d_model)

print(q.shape)
print(k.shape)
print(v.shape)
```

    torch.Size([10, 20, 512])
    torch.Size([10, 20, 512])
    torch.Size([10, 20, 512])


Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다.


```python
batch_size = q.shape[0]
d_k = d_model // num_heads

q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)

print(q.shape)
print(k.shape)
print(v.shape)
```

    torch.Size([10, 20, 8, 64])
    torch.Size([10, 20, 8, 64])
    torch.Size([10, 20, 8, 64])



```python
q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
v = v.transpose(1, 2)  # (B, num_heads, L, d_k)

print(q.shape)
print(k.shape)
print(v.shape)
```

    torch.Size([10, 8, 20, 64])
    torch.Size([10, 8, 20, 64])
    torch.Size([10, 8, 20, 64])


### **Scaled dot-product self-attention 구현**

각 head에서 실행되는 self-attetion 과정입니다.


```python
attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)

print(attn_dists)
print(attn_dists.shape)
```

    tensor([[[[0.0239, 0.0834, 0.0695,  ..., 0.0290, 0.0290, 0.0290],
              [0.0476, 0.0368, 0.0428,  ..., 0.0535, 0.0535, 0.0535],
              [0.0662, 0.0391, 0.0527,  ..., 0.0509, 0.0509, 0.0509],
              ...,
              [0.0263, 0.0439, 0.0686,  ..., 0.0512, 0.0512, 0.0512],
              [0.0263, 0.0439, 0.0686,  ..., 0.0512, 0.0512, 0.0512],
              [0.0263, 0.0439, 0.0686,  ..., 0.0512, 0.0512, 0.0512]],
    
             [[0.0310, 0.0974, 0.0420,  ..., 0.0358, 0.0358, 0.0358],
              [0.0693, 0.0407, 0.0387,  ..., 0.0566, 0.0566, 0.0566],
              [0.0415, 0.0247, 0.0337,  ..., 0.0552, 0.0552, 0.0552],
              ...,
              [0.0502, 0.1217, 0.0597,  ..., 0.0340, 0.0340, 0.0340],
              [0.0502, 0.1217, 0.0597,  ..., 0.0340, 0.0340, 0.0340],
              [0.0502, 0.1217, 0.0597,  ..., 0.0340, 0.0340, 0.0340]],
    
             [[0.0671, 0.0280, 0.0305,  ..., 0.0456, 0.0456, 0.0456],
              [0.0590, 0.0393, 0.0746,  ..., 0.0453, 0.0453, 0.0453],
              [0.0417, 0.0316, 0.0822,  ..., 0.0339, 0.0339, 0.0339],
              ...,
              [0.0356, 0.0630, 0.0263,  ..., 0.0434, 0.0434, 0.0434],
              [0.0356, 0.0630, 0.0263,  ..., 0.0434, 0.0434, 0.0434],
              [0.0356, 0.0630, 0.0263,  ..., 0.0434, 0.0434, 0.0434]],
    
             ...,
    
             [[0.0484, 0.0669, 0.0412,  ..., 0.0499, 0.0499, 0.0499],
              [0.0550, 0.0591, 0.0307,  ..., 0.0499, 0.0499, 0.0499],
              [0.0576, 0.0599, 0.0894,  ..., 0.0366, 0.0366, 0.0366],
              ...,
              [0.0531, 0.0328, 0.0459,  ..., 0.0659, 0.0659, 0.0659],
              [0.0531, 0.0328, 0.0459,  ..., 0.0659, 0.0659, 0.0659],
              [0.0531, 0.0328, 0.0459,  ..., 0.0659, 0.0659, 0.0659]],
    
             [[0.0569, 0.0901, 0.0312,  ..., 0.0320, 0.0320, 0.0320],
              [0.0504, 0.0432, 0.0407,  ..., 0.0727, 0.0727, 0.0727],
              [0.0447, 0.0945, 0.0392,  ..., 0.0582, 0.0582, 0.0582],
              ...,
              [0.0313, 0.0372, 0.0659,  ..., 0.0498, 0.0498, 0.0498],
              [0.0313, 0.0372, 0.0659,  ..., 0.0498, 0.0498, 0.0498],
              [0.0313, 0.0372, 0.0659,  ..., 0.0498, 0.0498, 0.0498]],
    
             [[0.0398, 0.0526, 0.0411,  ..., 0.0725, 0.0725, 0.0725],
              [0.0482, 0.0529, 0.0389,  ..., 0.0326, 0.0326, 0.0326],
              [0.0291, 0.0733, 0.0789,  ..., 0.0497, 0.0497, 0.0497],
              ...,
              [0.0478, 0.0535, 0.0496,  ..., 0.0281, 0.0281, 0.0281],
              [0.0478, 0.0535, 0.0496,  ..., 0.0281, 0.0281, 0.0281],
              [0.0478, 0.0535, 0.0496,  ..., 0.0281, 0.0281, 0.0281]]],
    
    
            [[[0.0446, 0.0155, 0.0216,  ..., 0.0574, 0.0574, 0.0574],
              [0.0339, 0.0507, 0.0422,  ..., 0.0504, 0.0504, 0.0504],
              [0.0316, 0.0633, 0.0387,  ..., 0.0495, 0.0495, 0.0495],
              ...,
              [0.0349, 0.0369, 0.0958,  ..., 0.0478, 0.0478, 0.0478],
              [0.0349, 0.0369, 0.0958,  ..., 0.0478, 0.0478, 0.0478],
              [0.0349, 0.0369, 0.0958,  ..., 0.0478, 0.0478, 0.0478]],
    
             [[0.0345, 0.1077, 0.0888,  ..., 0.0432, 0.0432, 0.0432],
              [0.0793, 0.0614, 0.0696,  ..., 0.0458, 0.0458, 0.0458],
              [0.0792, 0.0889, 0.0695,  ..., 0.0417, 0.0417, 0.0417],
              ...,
              [0.0670, 0.0729, 0.0781,  ..., 0.0447, 0.0447, 0.0447],
              [0.0670, 0.0729, 0.0781,  ..., 0.0447, 0.0447, 0.0447],
              [0.0670, 0.0729, 0.0781,  ..., 0.0447, 0.0447, 0.0447]],
    
             [[0.0655, 0.0352, 0.0641,  ..., 0.0506, 0.0506, 0.0506],
              [0.0775, 0.0773, 0.0433,  ..., 0.0444, 0.0444, 0.0444],
              [0.0493, 0.0736, 0.0324,  ..., 0.0514, 0.0514, 0.0514],
              ...,
              [0.0383, 0.0495, 0.0463,  ..., 0.0503, 0.0503, 0.0503],
              [0.0383, 0.0495, 0.0463,  ..., 0.0503, 0.0503, 0.0503],
              [0.0383, 0.0495, 0.0463,  ..., 0.0503, 0.0503, 0.0503]],
    
             ...,
    
             [[0.0210, 0.0579, 0.0313,  ..., 0.0546, 0.0546, 0.0546],
              [0.0341, 0.0934, 0.0365,  ..., 0.0479, 0.0479, 0.0479],
              [0.0560, 0.0697, 0.0783,  ..., 0.0474, 0.0474, 0.0474],
              ...,
              [0.0311, 0.0355, 0.0385,  ..., 0.0562, 0.0562, 0.0562],
              [0.0311, 0.0355, 0.0385,  ..., 0.0562, 0.0562, 0.0562],
              [0.0311, 0.0355, 0.0385,  ..., 0.0562, 0.0562, 0.0562]],
    
             [[0.0536, 0.0900, 0.0504,  ..., 0.0408, 0.0408, 0.0408],
              [0.0842, 0.0723, 0.0715,  ..., 0.0431, 0.0431, 0.0431],
              [0.0431, 0.0322, 0.0499,  ..., 0.0534, 0.0534, 0.0534],
              ...,
              [0.0779, 0.0315, 0.0245,  ..., 0.0506, 0.0506, 0.0506],
              [0.0779, 0.0315, 0.0245,  ..., 0.0506, 0.0506, 0.0506],
              [0.0779, 0.0315, 0.0245,  ..., 0.0506, 0.0506, 0.0506]],
    
             [[0.0392, 0.0487, 0.0797,  ..., 0.0470, 0.0470, 0.0470],
              [0.0848, 0.0552, 0.0598,  ..., 0.0443, 0.0443, 0.0443],
              [0.0530, 0.0661, 0.0734,  ..., 0.0458, 0.0458, 0.0458],
              ...,
              [0.0816, 0.0569, 0.0965,  ..., 0.0373, 0.0373, 0.0373],
              [0.0816, 0.0569, 0.0965,  ..., 0.0373, 0.0373, 0.0373],
              [0.0816, 0.0569, 0.0965,  ..., 0.0373, 0.0373, 0.0373]]],
    
    
            [[[0.0429, 0.0581, 0.0514,  ..., 0.0462, 0.0462, 0.0462],
              [0.0400, 0.0532, 0.0501,  ..., 0.0494, 0.0494, 0.0494],
              [0.0434, 0.0754, 0.0845,  ..., 0.0316, 0.0316, 0.0316],
              ...,
              [0.0434, 0.0263, 0.0765,  ..., 0.0549, 0.0549, 0.0549],
              [0.0434, 0.0263, 0.0765,  ..., 0.0549, 0.0549, 0.0549],
              [0.0434, 0.0263, 0.0765,  ..., 0.0549, 0.0549, 0.0549]],
    
             [[0.0818, 0.0520, 0.0632,  ..., 0.0409, 0.0409, 0.0409],
              [0.0584, 0.0381, 0.0412,  ..., 0.0558, 0.0558, 0.0558],
              [0.0709, 0.0839, 0.0527,  ..., 0.0392, 0.0392, 0.0392],
              ...,
              [0.0388, 0.0398, 0.1032,  ..., 0.0389, 0.0389, 0.0389],
              [0.0388, 0.0398, 0.1032,  ..., 0.0389, 0.0389, 0.0389],
              [0.0388, 0.0398, 0.1032,  ..., 0.0389, 0.0389, 0.0389]],
    
             [[0.0347, 0.0399, 0.0433,  ..., 0.0577, 0.0577, 0.0577],
              [0.0461, 0.0711, 0.0688,  ..., 0.0354, 0.0354, 0.0354],
              [0.0737, 0.0582, 0.0480,  ..., 0.0475, 0.0475, 0.0475],
              ...,
              [0.0419, 0.0364, 0.0505,  ..., 0.0490, 0.0490, 0.0490],
              [0.0419, 0.0364, 0.0505,  ..., 0.0490, 0.0490, 0.0490],
              [0.0419, 0.0364, 0.0505,  ..., 0.0490, 0.0490, 0.0490]],
    
             ...,
    
             [[0.0903, 0.0509, 0.0360,  ..., 0.0504, 0.0504, 0.0504],
              [0.0373, 0.1266, 0.0673,  ..., 0.0415, 0.0415, 0.0415],
              [0.0396, 0.0395, 0.0540,  ..., 0.0462, 0.0462, 0.0462],
              ...,
              [0.0340, 0.0433, 0.0275,  ..., 0.0659, 0.0659, 0.0659],
              [0.0340, 0.0433, 0.0275,  ..., 0.0659, 0.0659, 0.0659],
              [0.0340, 0.0433, 0.0275,  ..., 0.0659, 0.0659, 0.0659]],
    
             [[0.0430, 0.0464, 0.0644,  ..., 0.0575, 0.0575, 0.0575],
              [0.0480, 0.0698, 0.0425,  ..., 0.0486, 0.0486, 0.0486],
              [0.0689, 0.1312, 0.0890,  ..., 0.0307, 0.0307, 0.0307],
              ...,
              [0.0339, 0.0401, 0.0567,  ..., 0.0548, 0.0548, 0.0548],
              [0.0339, 0.0401, 0.0567,  ..., 0.0548, 0.0548, 0.0548],
              [0.0339, 0.0401, 0.0567,  ..., 0.0548, 0.0548, 0.0548]],
    
             [[0.0686, 0.0618, 0.0424,  ..., 0.0400, 0.0400, 0.0400],
              [0.0225, 0.0682, 0.0364,  ..., 0.0606, 0.0606, 0.0606],
              [0.0479, 0.0906, 0.0721,  ..., 0.0314, 0.0314, 0.0314],
              ...,
              [0.0538, 0.0789, 0.0557,  ..., 0.0296, 0.0296, 0.0296],
              [0.0538, 0.0789, 0.0557,  ..., 0.0296, 0.0296, 0.0296],
              [0.0538, 0.0789, 0.0557,  ..., 0.0296, 0.0296, 0.0296]]],
    
    
            ...,
    
    
            [[[0.0348, 0.0432, 0.0354,  ..., 0.0509, 0.0913, 0.0460],
              [0.0547, 0.0516, 0.0590,  ..., 0.1020, 0.0423, 0.0855],
              [0.0620, 0.0257, 0.0334,  ..., 0.0463, 0.0467, 0.0494],
              ...,
              [0.0369, 0.0418, 0.0491,  ..., 0.0485, 0.0567, 0.0432],
              [0.0607, 0.0720, 0.0805,  ..., 0.0720, 0.0208, 0.0528],
              [0.0291, 0.0274, 0.0430,  ..., 0.0400, 0.0408, 0.0383]],
    
             [[0.0831, 0.0375, 0.0357,  ..., 0.0476, 0.0431, 0.0455],
              [0.0774, 0.0392, 0.0320,  ..., 0.0497, 0.0421, 0.0399],
              [0.0352, 0.0526, 0.0542,  ..., 0.0285, 0.0234, 0.0374],
              ...,
              [0.0588, 0.0493, 0.0440,  ..., 0.0646, 0.0384, 0.0577],
              [0.0528, 0.0853, 0.0476,  ..., 0.0400, 0.0522, 0.0382],
              [0.0490, 0.0465, 0.0516,  ..., 0.0458, 0.0741, 0.0558]],
    
             [[0.0297, 0.0849, 0.0560,  ..., 0.0536, 0.0412, 0.0816],
              [0.0612, 0.0392, 0.0681,  ..., 0.0435, 0.0557, 0.0727],
              [0.0374, 0.0479, 0.0334,  ..., 0.0411, 0.0708, 0.0440],
              ...,
              [0.0516, 0.0872, 0.0394,  ..., 0.0775, 0.0513, 0.0617],
              [0.0429, 0.0632, 0.0412,  ..., 0.0719, 0.0518, 0.0519],
              [0.0894, 0.0368, 0.0565,  ..., 0.0395, 0.0499, 0.0546]],
    
             ...,
    
             [[0.0804, 0.0399, 0.0381,  ..., 0.0550, 0.0604, 0.0731],
              [0.0238, 0.0417, 0.0413,  ..., 0.0301, 0.0732, 0.0641],
              [0.0547, 0.0502, 0.0658,  ..., 0.0324, 0.0312, 0.0205],
              ...,
              [0.0585, 0.0261, 0.0331,  ..., 0.0334, 0.0403, 0.0268],
              [0.0618, 0.0499, 0.0479,  ..., 0.0393, 0.0520, 0.0872],
              [0.0867, 0.0619, 0.0294,  ..., 0.0299, 0.0343, 0.0325]],
    
             [[0.0225, 0.0484, 0.0236,  ..., 0.0343, 0.0173, 0.0535],
              [0.0430, 0.0414, 0.0957,  ..., 0.0449, 0.0531, 0.0598],
              [0.0422, 0.0579, 0.0741,  ..., 0.0534, 0.0260, 0.0340],
              ...,
              [0.0214, 0.0856, 0.0571,  ..., 0.0350, 0.0334, 0.0308],
              [0.0846, 0.0264, 0.0400,  ..., 0.0548, 0.0550, 0.0633],
              [0.0843, 0.0336, 0.0387,  ..., 0.0506, 0.0726, 0.0442]],
    
             [[0.0617, 0.0307, 0.0445,  ..., 0.0481, 0.0304, 0.0542],
              [0.0664, 0.0382, 0.0373,  ..., 0.0476, 0.0593, 0.0677],
              [0.0485, 0.0467, 0.0577,  ..., 0.0546, 0.0667, 0.0452],
              ...,
              [0.0399, 0.0440, 0.0404,  ..., 0.0585, 0.0836, 0.0522],
              [0.0353, 0.0519, 0.0349,  ..., 0.0434, 0.0608, 0.0563],
              [0.0340, 0.0876, 0.0296,  ..., 0.0464, 0.1066, 0.0473]]],
    
    
            [[[0.0566, 0.0580, 0.0576,  ..., 0.0378, 0.0378, 0.0378],
              [0.0461, 0.0404, 0.0458,  ..., 0.0543, 0.0543, 0.0543],
              [0.0472, 0.0305, 0.0447,  ..., 0.0748, 0.0748, 0.0748],
              ...,
              [0.0550, 0.0275, 0.0499,  ..., 0.0548, 0.0548, 0.0548],
              [0.0550, 0.0275, 0.0499,  ..., 0.0548, 0.0548, 0.0548],
              [0.0550, 0.0275, 0.0499,  ..., 0.0548, 0.0548, 0.0548]],
    
             [[0.0641, 0.0360, 0.0493,  ..., 0.0529, 0.0529, 0.0529],
              [0.0367, 0.0952, 0.0520,  ..., 0.0386, 0.0386, 0.0386],
              [0.0339, 0.0606, 0.0436,  ..., 0.0352, 0.0352, 0.0352],
              ...,
              [0.0476, 0.0353, 0.0628,  ..., 0.0338, 0.0338, 0.0338],
              [0.0476, 0.0353, 0.0628,  ..., 0.0338, 0.0338, 0.0338],
              [0.0476, 0.0353, 0.0628,  ..., 0.0338, 0.0338, 0.0338]],
    
             [[0.0507, 0.0453, 0.0409,  ..., 0.0615, 0.0615, 0.0615],
              [0.0615, 0.0784, 0.0624,  ..., 0.0578, 0.0578, 0.0578],
              [0.0425, 0.0508, 0.0285,  ..., 0.0466, 0.0466, 0.0466],
              ...,
              [0.0565, 0.0431, 0.0276,  ..., 0.0501, 0.0501, 0.0501],
              [0.0565, 0.0431, 0.0276,  ..., 0.0501, 0.0501, 0.0501],
              [0.0565, 0.0431, 0.0276,  ..., 0.0501, 0.0501, 0.0501]],
    
             ...,
    
             [[0.0563, 0.0542, 0.0954,  ..., 0.0440, 0.0440, 0.0440],
              [0.0271, 0.0420, 0.0326,  ..., 0.0452, 0.0452, 0.0452],
              [0.0585, 0.0431, 0.0473,  ..., 0.0375, 0.0375, 0.0375],
              ...,
              [0.0342, 0.0457, 0.0422,  ..., 0.0605, 0.0605, 0.0605],
              [0.0342, 0.0457, 0.0422,  ..., 0.0605, 0.0605, 0.0605],
              [0.0342, 0.0457, 0.0422,  ..., 0.0605, 0.0605, 0.0605]],
    
             [[0.0386, 0.0543, 0.0599,  ..., 0.0549, 0.0549, 0.0549],
              [0.0395, 0.0364, 0.0496,  ..., 0.0662, 0.0662, 0.0662],
              [0.0305, 0.0319, 0.0467,  ..., 0.0897, 0.0897, 0.0897],
              ...,
              [0.0496, 0.0384, 0.0391,  ..., 0.0602, 0.0602, 0.0602],
              [0.0496, 0.0384, 0.0391,  ..., 0.0602, 0.0602, 0.0602],
              [0.0496, 0.0384, 0.0391,  ..., 0.0602, 0.0602, 0.0602]],
    
             [[0.0538, 0.0766, 0.0336,  ..., 0.0375, 0.0375, 0.0375],
              [0.1003, 0.0366, 0.0482,  ..., 0.0338, 0.0338, 0.0338],
              [0.0325, 0.1130, 0.0657,  ..., 0.0236, 0.0236, 0.0236],
              ...,
              [0.0797, 0.0393, 0.0837,  ..., 0.0267, 0.0267, 0.0267],
              [0.0797, 0.0393, 0.0837,  ..., 0.0267, 0.0267, 0.0267],
              [0.0797, 0.0393, 0.0837,  ..., 0.0267, 0.0267, 0.0267]]],
    
    
            [[[0.0380, 0.0352, 0.0232,  ..., 0.0571, 0.0571, 0.0571],
              [0.0386, 0.0428, 0.0734,  ..., 0.0575, 0.0575, 0.0575],
              [0.0524, 0.0813, 0.0325,  ..., 0.0581, 0.0581, 0.0581],
              ...,
              [0.0445, 0.0254, 0.0263,  ..., 0.0505, 0.0505, 0.0505],
              [0.0445, 0.0254, 0.0263,  ..., 0.0505, 0.0505, 0.0505],
              [0.0445, 0.0254, 0.0263,  ..., 0.0505, 0.0505, 0.0505]],
    
             [[0.1064, 0.0549, 0.0354,  ..., 0.0398, 0.0398, 0.0398],
              [0.0393, 0.0897, 0.0525,  ..., 0.0363, 0.0363, 0.0363],
              [0.1127, 0.0669, 0.0500,  ..., 0.0372, 0.0372, 0.0372],
              ...,
              [0.0403, 0.0456, 0.0701,  ..., 0.0438, 0.0438, 0.0438],
              [0.0403, 0.0456, 0.0701,  ..., 0.0438, 0.0438, 0.0438],
              [0.0403, 0.0456, 0.0701,  ..., 0.0438, 0.0438, 0.0438]],
    
             [[0.0574, 0.0397, 0.0304,  ..., 0.0669, 0.0669, 0.0669],
              [0.0549, 0.0730, 0.0497,  ..., 0.0538, 0.0538, 0.0538],
              [0.0535, 0.0384, 0.0500,  ..., 0.0679, 0.0679, 0.0679],
              ...,
              [0.0279, 0.0476, 0.0492,  ..., 0.0553, 0.0553, 0.0553],
              [0.0279, 0.0476, 0.0492,  ..., 0.0553, 0.0553, 0.0553],
              [0.0279, 0.0476, 0.0492,  ..., 0.0553, 0.0553, 0.0553]],
    
             ...,
    
             [[0.0813, 0.0496, 0.0529,  ..., 0.0379, 0.0379, 0.0379],
              [0.0415, 0.0417, 0.0375,  ..., 0.0449, 0.0449, 0.0449],
              [0.0399, 0.0270, 0.0555,  ..., 0.0407, 0.0407, 0.0407],
              ...,
              [0.0580, 0.0485, 0.0346,  ..., 0.0643, 0.0643, 0.0643],
              [0.0580, 0.0485, 0.0346,  ..., 0.0643, 0.0643, 0.0643],
              [0.0580, 0.0485, 0.0346,  ..., 0.0643, 0.0643, 0.0643]],
    
             [[0.0582, 0.0588, 0.0373,  ..., 0.0317, 0.0317, 0.0317],
              [0.0467, 0.0321, 0.0727,  ..., 0.0585, 0.0585, 0.0585],
              [0.0203, 0.0376, 0.0222,  ..., 0.0708, 0.0708, 0.0708],
              ...,
              [0.0375, 0.0371, 0.0388,  ..., 0.0581, 0.0581, 0.0581],
              [0.0375, 0.0371, 0.0388,  ..., 0.0581, 0.0581, 0.0581],
              [0.0375, 0.0371, 0.0388,  ..., 0.0581, 0.0581, 0.0581]],
    
             [[0.0685, 0.0303, 0.0582,  ..., 0.0454, 0.0454, 0.0454],
              [0.0717, 0.0420, 0.0643,  ..., 0.0389, 0.0389, 0.0389],
              [0.0587, 0.0525, 0.0626,  ..., 0.0409, 0.0409, 0.0409],
              ...,
              [0.0383, 0.0405, 0.0924,  ..., 0.0276, 0.0276, 0.0276],
              [0.0383, 0.0405, 0.0924,  ..., 0.0276, 0.0276, 0.0276],
              [0.0383, 0.0405, 0.0924,  ..., 0.0276, 0.0276, 0.0276]]]],
           grad_fn=<SoftmaxBackward>)
    torch.Size([10, 8, 20, 20])



```python
attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)

print(attn_values.shape)
```

    torch.Size([10, 8, 20, 64])


### **각 head의 결과물 병합**

각 head의 결과물을 concat하고 동일 차원으로 linear transformation합니다.


```python
attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)
attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)

print(attn_values.shape)
```

    torch.Size([10, 20, 512])



```python
outputs = w_0(attn_values)

print(outputs)
print(outputs.shape)
```

    tensor([[[ 0.2059,  0.0120,  0.0695,  ...,  0.1047, -0.0467, -0.0168],
             [ 0.2283, -0.0258,  0.1169,  ...,  0.0617,  0.0260, -0.0507],
             [ 0.2439, -0.0454,  0.0774,  ...,  0.0944, -0.0305, -0.0307],
             ...,
             [ 0.1824, -0.0202,  0.0817,  ...,  0.0757,  0.0025, -0.0524],
             [ 0.1824, -0.0202,  0.0817,  ...,  0.0757,  0.0025, -0.0524],
             [ 0.1824, -0.0202,  0.0817,  ...,  0.0757,  0.0025, -0.0524]],
    
            [[ 0.3464, -0.0775,  0.2700,  ...,  0.1175,  0.2466, -0.1423],
             [ 0.3387, -0.0389,  0.2358,  ...,  0.1395,  0.2716, -0.1344],
             [ 0.3298, -0.0075,  0.2194,  ...,  0.1529,  0.2765, -0.1500],
             ...,
             [ 0.2907, -0.0419,  0.2149,  ...,  0.1152,  0.2891, -0.1068],
             [ 0.2907, -0.0419,  0.2149,  ...,  0.1152,  0.2891, -0.1068],
             [ 0.2907, -0.0419,  0.2149,  ...,  0.1152,  0.2891, -0.1068]],
    
            [[ 0.1277,  0.0274,  0.1202,  ...,  0.0119,  0.1627, -0.0047],
             [ 0.1778,  0.0530,  0.1497,  ...,  0.0879,  0.1104, -0.0655],
             [ 0.1507,  0.0698,  0.0823,  ..., -0.0281,  0.1140,  0.0093],
             ...,
             [ 0.1264,  0.0577,  0.0336,  ...,  0.0388,  0.1301,  0.0035],
             [ 0.1264,  0.0577,  0.0336,  ...,  0.0388,  0.1301,  0.0035],
             [ 0.1264,  0.0577,  0.0336,  ...,  0.0388,  0.1301,  0.0035]],
    
            ...,
    
            [[-0.0751,  0.1241,  0.0471,  ..., -0.2152,  0.0605,  0.0719],
             [-0.0912,  0.1062,  0.0444,  ..., -0.1460,  0.0244,  0.0451],
             [-0.0119,  0.1421,  0.0748,  ..., -0.1669,  0.0161,  0.0193],
             ...,
             [-0.0208,  0.1365,  0.0516,  ..., -0.1844,  0.0370, -0.0088],
             [-0.0659,  0.1248,  0.1109,  ..., -0.2105,  0.0691,  0.0219],
             [-0.0162,  0.1228,  0.1046,  ..., -0.2048,  0.0038,  0.0416]],
    
            [[-0.0062, -0.0592,  0.1552,  ...,  0.1261,  0.1316, -0.0245],
             [-0.0147, -0.1135,  0.1938,  ...,  0.1470,  0.1144, -0.0396],
             [-0.0133, -0.0719,  0.1307,  ...,  0.1041,  0.1846, -0.0987],
             ...,
             [-0.0483, -0.0655,  0.1481,  ...,  0.1241,  0.1212, -0.0497],
             [-0.0483, -0.0655,  0.1481,  ...,  0.1241,  0.1212, -0.0497],
             [-0.0483, -0.0655,  0.1481,  ...,  0.1241,  0.1212, -0.0497]],
    
            [[ 0.1172, -0.0519,  0.2362,  ...,  0.0433,  0.0142, -0.0574],
             [ 0.0845, -0.0429,  0.2107,  ...,  0.0087,  0.0273, -0.0784],
             [ 0.0839, -0.0808,  0.2263,  ..., -0.0242,  0.0798, -0.1087],
             ...,
             [ 0.0428, -0.0266,  0.1910,  ..., -0.0071,  0.0941, -0.0802],
             [ 0.0428, -0.0266,  0.1910,  ..., -0.0071,  0.0941, -0.0802],
             [ 0.0428, -0.0266,  0.1910,  ..., -0.0071,  0.0941, -0.0802]]],
           grad_fn=<AddBackward0>)
    torch.Size([10, 20, 512])


### **전체 코드**

위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현하겠습니다.


```python
class MultiheadAttention(nn.Module):
  def __init__(self):
    super(MultiheadAttention, self).__init__()

    # Q, K, V learnable matrices
    self.w_q = nn.Linear(d_model, d_model)
    self.w_k = nn.Linear(d_model, d_model)
    self.w_v = nn.Linear(d_model, d_model)

    # Linear transformation for concatenated outputs
    self.w_0 = nn.Linear(d_model, d_model)

  def forward(self, q, k, v):
    batch_size = q.shape[0]

    q = self.w_q(q)  # (B, L, d_model)
    k = self.w_k(k)  # (B, L, d_model)
    v = self.w_v(v)  # (B, L, d_model)

    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)

    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)

    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)
    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)

    return self.w_0(attn_values)

  def self_attention(self, q, k, v):
    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)

    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)

    return attn_values
```


```python
multihead_attn = MultiheadAttention()

outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)
```


```python
print(outputs)
print(outputs.shape)
```

    tensor([[[-7.4790e-02,  1.2649e-01,  2.7898e-02,  ...,  2.8898e-03,
              -1.1881e-01, -1.4657e-01],
             [-6.8472e-02,  9.4750e-02, -1.0707e-04,  ..., -1.0426e-03,
              -6.8871e-02, -9.8160e-02],
             [-1.2238e-01,  8.9813e-02,  4.9224e-02,  ..., -1.2461e-02,
              -1.0624e-01, -9.2314e-02],
             ...,
             [-8.2820e-02,  1.4484e-01,  4.0152e-02,  ..., -3.9796e-02,
              -8.1333e-02, -1.4834e-01],
             [-8.2820e-02,  1.4484e-01,  4.0151e-02,  ..., -3.9796e-02,
              -8.1333e-02, -1.4834e-01],
             [-8.2820e-02,  1.4484e-01,  4.0151e-02,  ..., -3.9796e-02,
              -8.1333e-02, -1.4834e-01]],
    
            [[-1.9506e-01,  1.7586e-01,  1.1148e-01,  ...,  6.6061e-02,
              -3.0465e-01, -1.7430e-01],
             [-1.8805e-01,  1.7777e-01,  1.0263e-01,  ...,  4.8435e-02,
              -3.2742e-01, -1.6446e-01],
             [-1.9491e-01,  1.7369e-01,  1.2551e-01,  ...,  7.3464e-02,
              -2.9549e-01, -2.0101e-01],
             ...,
             [-2.1391e-01,  2.0415e-01,  8.1248e-02,  ...,  4.3002e-02,
              -2.9454e-01, -1.7793e-01],
             [-2.1391e-01,  2.0415e-01,  8.1248e-02,  ...,  4.3002e-02,
              -2.9454e-01, -1.7793e-01],
             [-2.1391e-01,  2.0415e-01,  8.1248e-02,  ...,  4.3002e-02,
              -2.9454e-01, -1.7793e-01]],
    
            [[-1.1629e-01,  2.3173e-02,  1.4267e-01,  ..., -7.5987e-03,
              -4.9757e-02, -1.3998e-01],
             [-6.5609e-02, -2.1628e-02,  1.4489e-01,  ...,  5.7000e-02,
              -6.9388e-02, -1.1221e-01],
             [-1.0251e-01, -9.4326e-03,  1.4058e-01,  ...,  4.2453e-02,
              -5.1714e-02, -9.9830e-02],
             ...,
             [-7.5098e-02,  4.2783e-02,  8.6499e-02,  ...,  1.8369e-02,
              -1.0029e-01, -1.6194e-01],
             [-7.5098e-02,  4.2783e-02,  8.6499e-02,  ...,  1.8369e-02,
              -1.0029e-01, -1.6194e-01],
             [-7.5098e-02,  4.2783e-02,  8.6499e-02,  ...,  1.8369e-02,
              -1.0029e-01, -1.6194e-01]],
    
            ...,
    
            [[ 1.6563e-01, -1.5060e-03, -9.3419e-02,  ..., -5.7532e-02,
               8.3138e-02,  7.9686e-02],
             [ 1.5690e-01, -4.1409e-02, -1.3175e-01,  ..., -1.6118e-02,
               9.7021e-02,  7.2846e-02],
             [ 1.5184e-01, -3.0740e-02, -8.3715e-02,  ..., -5.1400e-02,
               1.1427e-01,  5.3179e-02],
             ...,
             [ 1.3819e-01,  3.0038e-03, -7.0018e-02,  ..., -4.1763e-02,
               1.1215e-01,  5.1411e-02],
             [ 1.6058e-01, -1.7930e-02, -1.5314e-01,  ...,  9.8441e-03,
               1.0156e-01,  2.8656e-02],
             [ 1.6694e-01, -2.8633e-02, -1.1788e-01,  ..., -4.6555e-02,
               1.2150e-01,  5.4781e-02]],
    
            [[ 8.9245e-02, -4.5693e-02, -7.3960e-02,  ...,  3.1406e-03,
              -1.2022e-01,  2.7886e-02],
             [ 6.8672e-02,  3.3401e-02, -5.3376e-02,  ...,  5.2078e-03,
              -1.1519e-01, -5.1573e-03],
             [ 5.5954e-02, -1.7646e-02, -8.0079e-02,  ..., -1.5568e-02,
              -1.2773e-01, -4.5706e-02],
             ...,
             [ 1.0485e-01,  5.3432e-02, -9.8003e-02,  ..., -4.3284e-02,
              -1.3665e-01, -9.7963e-03],
             [ 1.0485e-01,  5.3432e-02, -9.8003e-02,  ..., -4.3284e-02,
              -1.3665e-01, -9.7963e-03],
             [ 1.0485e-01,  5.3432e-02, -9.8003e-02,  ..., -4.3284e-02,
              -1.3665e-01, -9.7963e-03]],
    
            [[-6.1092e-03,  1.5332e-01, -3.2028e-02,  ...,  4.9739e-03,
              -1.2630e-01, -1.1733e-01],
             [-1.2762e-02,  1.3284e-01, -3.8760e-02,  ..., -3.6265e-02,
              -1.2219e-01, -1.0385e-01],
             [ 2.7796e-03,  1.3067e-01, -1.1663e-01,  ..., -6.4158e-02,
              -8.8693e-02, -1.3189e-01],
             ...,
             [ 2.3022e-02,  1.4613e-01, -8.5376e-02,  ..., -8.9085e-02,
              -1.2255e-01, -1.4669e-01],
             [ 2.3022e-02,  1.4613e-01, -8.5376e-02,  ..., -8.9085e-02,
              -1.2255e-01, -1.4669e-01],
             [ 2.3022e-02,  1.4613e-01, -8.5376e-02,  ..., -8.9085e-02,
              -1.2255e-01, -1.4669e-01]]], grad_fn=<AddBackward0>)
    torch.Size([10, 20, 512])



```python

```
