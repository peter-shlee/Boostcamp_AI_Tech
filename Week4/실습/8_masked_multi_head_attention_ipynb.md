##**8. Masked Multi-head Attention**
1. Masked Multi-head Attention 구현.
2. Encoder-Decoder Attention 구현.

### **필요 패키지 import**


```python
from torch import nn
from torch.nn import functional as F
from tqdm import tqdm

import torch
import math
```

### **데이터 전처리**

데이터의 값과 형태를 좀 더 명확하게 보기 위해 sample을 줄이겠습니다.


```python
pad_id = 0
vocab_size = 100

data = [
  [62, 13, 47, 39, 78, 33, 56, 13],
  [60, 96, 51, 32, 90],
  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],
  [66, 88, 98, 47],
  [77, 65, 51, 77, 19, 15, 35, 19, 23]
]
```


```python
def padding(data):
  max_len = len(max(data, key=len))
  print(f"Maximum sequence length: {max_len}")

  for i, seq in enumerate(tqdm(data)):
    if len(seq) < max_len:
      data[i] = seq + [pad_id] * (max_len - len(seq))

  return data, max_len
```


```python
data, max_len = padding(data)
```

    100%|██████████| 5/5 [00:00<00:00, 2856.38it/s]

    Maximum sequence length: 10


    



```python
data
```




    [[62, 13, 47, 39, 78, 33, 56, 13, 0, 0],
     [60, 96, 51, 32, 90, 0, 0, 0, 0, 0],
     [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],
     [66, 88, 98, 47, 0, 0, 0, 0, 0, 0],
     [77, 65, 51, 77, 19, 15, 35, 19, 23, 0]]



### **Hyperparameter 세팅 및 embedding**


```python
d_model = 8  # model의 hidden size
num_heads = 2  # head의 개수
inf = 1e12
```


```python
embedding = nn.Embedding(vocab_size, d_model)

# B: batch size, L: maximum sequence length
batch = torch.LongTensor(data)  # (B, L)
batch_emb = embedding(batch)  # (B, L, d_model)
```


```python
print(batch_emb)
print(batch_emb.shape)
```

    tensor([[[ 0.2550, -0.4428,  0.7346, -0.3933, -0.2046,  0.2942, -0.2660,
              -1.1428],
             [-1.0352,  0.3275,  1.7485,  0.8076, -0.2484, -1.6768, -1.4299,
              -0.0421],
             [ 0.1189,  0.3750,  0.6616, -1.3503,  0.1865,  1.0749, -0.0473,
              -1.1671],
             [ 0.2947,  0.2748,  0.0799, -1.0070, -0.4401,  0.1000, -0.1701,
               0.3335],
             [ 0.8617, -1.8479, -0.2741,  1.5330,  2.1181, -1.6514,  2.5777,
               0.4792],
             [-0.2381, -1.7446,  0.9682,  0.3277, -1.4323, -0.5189, -1.5919,
               0.5575],
             [-0.0319,  0.6477,  0.1013,  0.2387,  1.4991, -0.4776,  0.9203,
              -0.0516],
             [-1.0352,  0.3275,  1.7485,  0.8076, -0.2484, -1.6768, -1.4299,
              -0.0421],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901]],
    
            [[ 1.2196, -1.0349,  0.1615, -1.4158,  0.1854, -0.1793, -1.0174,
               1.0728],
             [ 1.4196, -0.5101,  0.6686,  0.4929, -0.6600, -2.0979,  0.3449,
               0.4479],
             [ 0.0112,  0.5996,  0.6688,  0.8247, -2.2550, -0.0732, -1.0623,
              -1.2935],
             [ 0.7508,  1.1867, -1.6139, -0.8396,  0.4372,  0.3599, -1.3519,
              -0.1466],
             [-1.2490, -1.9854,  0.8795, -1.4512,  1.6731, -0.3041,  1.2072,
               0.0174],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901]],
    
            [[-1.1630,  0.2321,  0.4831, -1.6985,  0.0389,  0.9595,  1.7281,
              -1.7526],
             [ 2.4404, -0.4142, -0.5684,  0.4567, -1.3346,  0.2988,  0.5340,
               1.2996],
             [-0.5535,  1.3327,  1.2649,  1.2043,  0.4918,  2.8021, -0.2270,
              -0.4126],
             [ 1.2976,  0.2208, -0.1368,  1.3249,  0.3557,  0.3101,  1.0532,
              -0.6079],
             [ 0.7079,  0.3220, -0.6783,  0.4513,  1.4247, -0.5088,  0.7577,
               0.1523],
             [-1.4251, -0.2316, -1.5436,  0.8820,  0.4927, -0.7052,  0.1159,
              -0.3948],
             [-1.3910,  1.1359, -0.2716, -1.3412,  0.2407,  1.1920,  0.2165,
               0.5876],
             [ 1.0910, -0.5126,  0.8378, -0.1675,  2.1423, -0.0680,  1.4086,
              -1.3506],
             [-0.7824, -2.6431,  0.1689, -0.7316, -1.4918, -0.3900, -0.5275,
               1.8768],
             [ 1.4505,  1.0266,  0.5883, -0.2152, -1.2988,  0.4511, -0.9287,
               0.5476]],
    
            [[-1.1481, -0.8084, -0.9181,  0.1362,  1.6929, -0.4642,  0.0106,
               0.4022],
             [ 0.8514, -0.7090,  0.7996, -1.2066, -1.7581, -1.4184,  0.3938,
              -0.1608],
             [ 1.0400,  0.3647, -0.8260,  0.1861, -0.3344,  0.4007,  0.3660,
              -0.0952],
             [ 0.1189,  0.3750,  0.6616, -1.3503,  0.1865,  1.0749, -0.0473,
              -1.1671],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901]],
    
            [[ 0.6355, -0.4910, -0.7652, -0.3491, -0.3123,  0.9576,  0.3051,
               0.9045],
             [ 1.2976,  0.2208, -0.1368,  1.3249,  0.3557,  0.3101,  1.0532,
              -0.6079],
             [ 0.0112,  0.5996,  0.6688,  0.8247, -2.2550, -0.0732, -1.0623,
              -1.2935],
             [ 0.6355, -0.4910, -0.7652, -0.3491, -0.3123,  0.9576,  0.3051,
               0.9045],
             [-0.5127, -0.2168,  1.1947, -1.2043,  0.9952,  1.7654,  0.5221,
               0.7917],
             [ 1.0382,  0.2767,  0.7835, -1.0597,  0.0951,  0.3053, -1.8149,
              -0.2771],
             [-1.1630,  0.2321,  0.4831, -1.6985,  0.0389,  0.9595,  1.7281,
              -1.7526],
             [-0.5127, -0.2168,  1.1947, -1.2043,  0.9952,  1.7654,  0.5221,
               0.7917],
             [-0.9972,  0.2873, -0.9026, -0.3610,  0.4762, -0.5578, -1.7903,
               0.6654],
             [-0.8794, -0.1781,  0.4248,  1.8430, -0.7313, -0.0866,  0.0170,
               0.9901]]], grad_fn=<EmbeddingBackward>)
    torch.Size([5, 10, 8])


### **Mask 구축**

`True`는 attention이 적용될 부분, `False`는 masking될 자리입니다.


```python
padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)

print(padding_mask)
print(padding_mask.shape)
```

    tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False]],
    
            [[ True,  True,  True,  True,  True, False, False, False, False, False]],
    
            [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],
    
            [[ True,  True,  True,  True, False, False, False, False, False, False]],
    
            [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])
    torch.Size([5, 1, 10])



```python
nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)
nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)

print(nopeak_mask)
print(nopeak_mask.shape)
```

    tensor([[[ True, False, False, False, False, False, False, False, False, False],
             [ True,  True, False, False, False, False, False, False, False, False],
             [ True,  True,  True, False, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True,  True, False, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])
    torch.Size([1, 10, 10])



```python
mask = padding_mask & nopeak_mask  # (B, L, L)

print(mask)
print(mask.shape)
```

    tensor([[[ True, False, False, False, False, False, False, False, False, False],
             [ True,  True, False, False, False, False, False, False, False, False],
             [ True,  True,  True, False, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True,  True, False, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],
    
            [[ True, False, False, False, False, False, False, False, False, False],
             [ True,  True, False, False, False, False, False, False, False, False],
             [ True,  True,  True, False, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False]],
    
            [[ True, False, False, False, False, False, False, False, False, False],
             [ True,  True, False, False, False, False, False, False, False, False],
             [ True,  True,  True, False, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True,  True, False, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],
    
            [[ True, False, False, False, False, False, False, False, False, False],
             [ True,  True, False, False, False, False, False, False, False, False],
             [ True,  True,  True, False, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False]],
    
            [[ True, False, False, False, False, False, False, False, False, False],
             [ True,  True, False, False, False, False, False, False, False, False],
             [ True,  True,  True, False, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True,  True, False, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])
    torch.Size([5, 10, 10])


### **Linear transformation & 여러 head로 나누기**


```python
w_q = nn.Linear(d_model, d_model)
w_k = nn.Linear(d_model, d_model)
w_v = nn.Linear(d_model, d_model)

w_0 = nn.Linear(d_model, d_model)
```


```python
q = w_q(batch_emb)  # (B, L, d_model)
k = w_k(batch_emb)  # (B, L, d_model)
v = w_v(batch_emb)  # (B, L, d_model)

batch_size = q.shape[0]
d_k = d_model // num_heads

q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)

q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
v = v.transpose(1, 2)  # (B, num_heads, L, d_k)

print(q.shape)
print(k.shape)
print(v.shape)
```

    torch.Size([5, 2, 10, 4])
    torch.Size([5, 2, 10, 4])
    torch.Size([5, 2, 10, 4])


### **Masking이 적용된 self-attention 구현**


```python
attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
```


```python
masks = mask.unsqueeze(1)  # (B, 1, L, L)
masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L)

print(masked_attn_scores)
print(masked_attn_scores.shape)
```

    tensor([[[[ 1.1358e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 3.8955e-02,  2.3423e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-8.2324e-02,  1.2975e-01,  5.6317e-02, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.3190e-02,  1.9183e-01, -8.9570e-02,  1.1764e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 3.0270e-01, -3.3662e-01, -3.2555e-01,  3.1001e-01,  1.1346e+00,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.3751e-01,  3.8373e-01, -4.4411e-02, -3.4071e-02,  5.8308e-01,
                4.0050e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 3.4144e-02, -1.6962e-01, -3.7383e-02,  4.9906e-02,  1.9644e-01,
                1.7909e-01, -6.4981e-03, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 3.8955e-02,  2.3423e-01,  1.2900e-01, -1.0449e-01,  1.0583e-01,
               -2.1770e-01,  1.3813e-01,  2.3423e-01, -1.0000e+12, -1.0000e+12],
              [ 8.3937e-02,  2.3626e-01,  1.4109e-02, -4.8650e-02,  3.8126e-01,
                1.4181e-01,  6.8896e-02,  2.3626e-01, -1.0000e+12, -1.0000e+12],
              [ 8.3937e-02,  2.3626e-01,  1.4109e-02, -4.8650e-02,  3.8126e-01,
                1.4181e-01,  6.8896e-02,  2.3626e-01, -1.0000e+12, -1.0000e+12]],
    
             [[-1.2620e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.8032e-01,  2.3103e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-7.8961e-02, -4.3421e-02, -1.9011e-02, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.8240e-02,  8.7153e-02, -5.3572e-03, -7.5801e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.1916e-01,  5.8444e-01, -6.5959e-01, -3.6523e-01, -2.5327e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.9992e-01,  7.2553e-03,  2.1968e-01,  1.0679e-01,  1.1068e+00,
               -2.8068e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.1243e-01,  4.0451e-01, -5.4133e-01, -3.1763e-01, -4.5581e-01,
                5.7935e-01, -5.1959e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.8032e-01,  2.3103e-01, -4.9863e-01, -1.9869e-01,  8.7901e-01,
                2.1655e-01,  1.5250e-01,  2.3103e-01, -1.0000e+12, -1.0000e+12],
              [-2.8303e-02,  6.1608e-01, -5.3579e-01, -3.7394e-01,  1.1896e+00,
                3.8325e-01,  2.0928e-01,  6.1608e-01, -1.0000e+12, -1.0000e+12],
              [-2.8303e-02,  6.1608e-01, -5.3579e-01, -3.7394e-01,  1.1896e+00,
                3.8325e-01,  2.0928e-01,  6.1608e-01, -1.0000e+12, -1.0000e+12]]],
    
    
            [[[ 5.4761e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 6.8830e-01,  8.5683e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.5266e-01, -1.0385e-01, -2.5393e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3889e-01, -2.5806e-01, -9.8524e-02,  1.0882e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 7.4356e-01,  3.8959e-01, -1.2252e-01,  2.1501e-01,  1.5049e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 5.2018e-01,  2.6782e-01, -5.1146e-01, -6.9328e-02,  6.1681e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 5.2018e-01,  2.6782e-01, -5.1146e-01, -6.9328e-02,  6.1681e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 5.2018e-01,  2.6782e-01, -5.1146e-01, -6.9328e-02,  6.1681e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 5.2018e-01,  2.6782e-01, -5.1146e-01, -6.9328e-02,  6.1681e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 5.2018e-01,  2.6782e-01, -5.1146e-01, -6.9328e-02,  6.1681e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],
    
             [[ 9.1821e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.7563e-01,  3.4378e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3945e-01,  5.6681e-01, -2.0420e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.5145e-02,  1.1260e-01, -7.0529e-02, -2.4056e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.8456e-01, -6.5309e-01,  4.1942e-01,  5.4247e-01, -3.9807e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3855e-01,  6.5420e-01,  9.5183e-02, -6.6394e-01,  9.8376e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3855e-01,  6.5420e-01,  9.5183e-02, -6.6394e-01,  9.8376e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3855e-01,  6.5420e-01,  9.5183e-02, -6.6394e-01,  9.8376e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3855e-01,  6.5420e-01,  9.5183e-02, -6.6394e-01,  9.8376e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3855e-01,  6.5420e-01,  9.5183e-02, -6.6394e-01,  9.8376e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],
    
    
            [[[ 5.1823e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-7.6867e-01,  1.6064e+00, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 7.7319e-01, -1.5332e+00, -3.3269e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-3.5902e-01,  7.8656e-01,  2.7693e-01,  3.6824e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-4.9665e-01,  1.0555e+00,  4.8713e-01,  6.0048e-01,  3.1931e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3800e-01, -1.5398e-01, -1.3539e-01,  1.6871e-02,  3.6577e-01,
                2.0105e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 5.4741e-01, -8.8777e-01, -1.9449e-01, -3.8801e-01, -2.0353e-01,
               -9.7749e-02,  1.9776e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-4.8403e-01,  1.0477e+00,  4.4196e-01,  5.1140e-01,  1.9083e-01,
                1.9095e-01, -7.3103e-02, -1.8508e-01, -1.0000e+12, -1.0000e+12],
              [-8.3753e-01,  3.7185e-01, -1.0457e+00, -2.2196e-01, -1.4229e-02,
               -6.0877e-01, -9.1136e-01,  4.3395e-01,  8.8054e-01, -1.0000e+12],
              [ 1.4093e-01, -1.3877e-03, -1.2776e-01, -9.2746e-02, -2.6529e-01,
               -2.7369e-01, -1.0057e-01, -1.9640e-01,  1.1396e-01,  6.1609e-02]],
    
             [[-6.0188e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-7.5772e-01,  1.1404e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.1965e+00, -4.5713e-02, -1.5482e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.4905e+00,  1.6600e-01, -3.3728e-01, -2.4587e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-8.5413e-01, -3.6579e-03, -6.3509e-02, -1.7352e-01, -2.9444e-01,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.0764e-01, -6.0674e-02,  1.5668e-01,  4.3190e-01,  3.9954e-01,
                2.8248e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 3.8572e-01, -4.9631e-01,  2.6989e-01, -1.6384e-01, -3.6802e-01,
               -3.1878e-01, -8.1368e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.2219e+00, -5.9650e-02, -2.5220e-02, -7.5026e-01, -8.4190e-01,
               -4.1004e-01, -3.1104e-01, -1.4403e+00, -1.0000e+12, -1.0000e+12],
              [ 1.4429e+00, -3.0676e-01,  3.9978e-01,  5.7150e-01,  5.9481e-01,
                2.5737e-01,  4.4143e-01,  1.4422e+00, -7.0415e-01, -1.0000e+12],
              [-7.3151e-01,  1.7181e-01, -3.8815e-01, -4.3742e-02, -1.7440e-01,
               -9.8639e-02, -6.1076e-01, -3.9034e-01,  2.6190e-01, -1.9009e-01]]],
    
    
            [[[ 7.2354e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-7.5628e-01,  9.4923e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.1190e-01,  1.3819e-01,  2.1935e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.3531e-01,  5.8322e-02, -2.4542e-01,  5.6317e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.0098e-01,  1.7845e-01, -3.4453e-01,  1.4109e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.0098e-01,  1.7845e-01, -3.4453e-01,  1.4109e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.0098e-01,  1.7845e-01, -3.4453e-01,  1.4109e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.0098e-01,  1.7845e-01, -3.4453e-01,  1.4109e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.0098e-01,  1.7845e-01, -3.4453e-01,  1.4109e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.0098e-01,  1.7845e-01, -3.4453e-01,  1.4109e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],
    
             [[ 2.4438e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.6468e-01,  4.4588e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.6656e-01, -4.2046e-02, -2.2544e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-4.1588e-01, -7.3972e-02, -2.2703e-01, -1.9011e-02, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.9336e-01, -1.2931e-02, -1.4317e-01, -5.3579e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.9336e-01, -1.2931e-02, -1.4317e-01, -5.3579e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.9336e-01, -1.2931e-02, -1.4317e-01, -5.3579e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.9336e-01, -1.2931e-02, -1.4317e-01, -5.3579e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.9336e-01, -1.2931e-02, -1.4317e-01, -5.3579e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.9336e-01, -1.2931e-02, -1.4317e-01, -5.3579e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],
    
    
            [[[ 2.5954e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 4.6056e-01,  3.6824e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-8.1557e-01, -7.1533e-01, -2.5393e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 2.5954e-01,  2.3202e-01,  1.3787e-01,  2.5954e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.7286e-01, -1.7827e-01, -1.1995e-01, -2.7286e-01, -1.6194e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-1.2537e-01, -1.3458e-02, -1.6872e-01, -1.2537e-01,  6.9435e-02,
               -1.2743e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-5.9213e-01, -5.6020e-01,  2.9304e-01, -5.9213e-01, -2.8197e-01,
               -1.7419e-01,  5.1823e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.7286e-01, -1.7827e-01, -1.1995e-01, -2.7286e-01, -1.6194e-02,
               -7.2000e-02,  9.3503e-02, -1.6194e-02, -1.0000e+12, -1.0000e+12],
              [-8.2946e-02, -2.7707e-02, -7.1104e-01, -8.2946e-02,  2.1580e-01,
                1.8349e-01, -1.0115e-02,  2.1580e-01,  2.5671e-01, -1.0000e+12],
              [-2.8459e-01, -3.0337e-01, -5.1146e-01, -2.8459e-01,  1.7655e-02,
                3.6232e-01, -2.3662e-01,  1.7655e-02,  1.0515e-01, -1.0000e+12]],
    
             [[-2.2081e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.9196e-01, -2.4587e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-9.0579e-02,  2.4542e-01, -2.0420e-01, -1.0000e+12, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-2.2081e-01,  4.7409e-02,  1.7720e-01, -2.2081e-01, -1.0000e+12,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-3.1074e-01, -4.6314e-01,  4.4670e-01, -3.1074e-01,  1.3759e-02,
               -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [ 1.2113e-01, -8.2958e-02, -1.2391e-01,  1.2113e-01,  8.6629e-03,
               -7.3182e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-3.5970e-01, -6.8319e-01,  4.4548e-01, -3.5970e-01, -7.1171e-02,
                3.2554e-01, -6.0188e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12],
              [-3.1074e-01, -4.6314e-01,  4.4670e-01, -3.1074e-01,  1.3759e-02,
                4.1948e-01,  3.4593e-02,  1.3759e-02, -1.0000e+12, -1.0000e+12],
              [ 9.8812e-02,  5.1819e-01, -4.6079e-02,  9.8812e-02,  3.1672e-01,
                1.6347e-01,  7.5810e-01,  3.1672e-01,  2.3207e-02, -1.0000e+12],
              [-2.9499e-01,  3.9645e-01,  9.5183e-02, -2.9499e-01, -4.1823e-01,
               -3.3562e-01, -6.0533e-01, -4.1823e-01, -2.1665e-01, -1.0000e+12]]]],
           grad_fn=<MaskedFillBackward0>)
    torch.Size([5, 2, 10, 10])


`-1* inf`로 masking된 부분은 softmax 후 0이 됩니다.


```python
attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L)

print(attn_dists)
print(attn_dists.shape)
```

    tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.4513, 0.5487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2954, 0.3652, 0.3394, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2393, 0.2937, 0.2217, 0.2453, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1864, 0.0983, 0.0994, 0.1877, 0.4282, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1467, 0.1876, 0.1223, 0.1236, 0.2290, 0.1908, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1417, 0.1156, 0.1320, 0.1440, 0.1667, 0.1639, 0.1361, 0.0000,
               0.0000, 0.0000],
              [0.1199, 0.1458, 0.1312, 0.1039, 0.1282, 0.0928, 0.1324, 0.1458,
               0.0000, 0.0000],
              [0.1173, 0.1365, 0.1093, 0.1027, 0.1579, 0.1242, 0.1155, 0.1365,
               0.0000, 0.0000],
              [0.1173, 0.1365, 0.1093, 0.1027, 0.1579, 0.1242, 0.1155, 0.1365,
               0.0000, 0.0000]],
    
             [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3228, 0.3345, 0.3427, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2545, 0.2700, 0.2461, 0.2294, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1752, 0.3913, 0.1128, 0.1514, 0.1693, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1460, 0.1204, 0.1489, 0.1330, 0.3615, 0.0903, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1220, 0.2260, 0.0878, 0.1098, 0.0956, 0.2692, 0.0897, 0.0000,
               0.0000, 0.0000],
              [0.0870, 0.1313, 0.0633, 0.0854, 0.2510, 0.1294, 0.1214, 0.1313,
               0.0000, 0.0000],
              [0.0815, 0.1552, 0.0490, 0.0577, 0.2753, 0.1229, 0.1033, 0.1552,
               0.0000, 0.0000],
              [0.0815, 0.1552, 0.0490, 0.0577, 0.2753, 0.1229, 0.1033, 0.1552,
               0.0000, 0.0000]]],
    
    
            [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.4580, 0.5420, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3165, 0.3673, 0.3161, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2199, 0.2157, 0.2530, 0.3113, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3063, 0.2150, 0.1288, 0.1806, 0.1693, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2639, 0.2050, 0.0941, 0.1464, 0.2907, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2639, 0.2050, 0.0941, 0.1464, 0.2907, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2639, 0.2050, 0.0941, 0.1464, 0.2907, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2639, 0.2050, 0.0941, 0.1464, 0.2907, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2639, 0.2050, 0.0941, 0.1464, 0.2907, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000]],
    
             [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3730, 0.6270, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2339, 0.5238, 0.2423, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2436, 0.2796, 0.2328, 0.2439, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2030, 0.0878, 0.2567, 0.2903, 0.1622, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1451, 0.3543, 0.2026, 0.0948, 0.2032, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1451, 0.3543, 0.2026, 0.0948, 0.2032, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1451, 0.3543, 0.2026, 0.0948, 0.2032, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1451, 0.3543, 0.2026, 0.0948, 0.2032, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1451, 0.3543, 0.2026, 0.0948, 0.2032, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000]]],
    
    
            [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.0851, 0.9149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.6990, 0.0696, 0.2313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1234, 0.3881, 0.2331, 0.2554, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.0732, 0.3458, 0.1959, 0.2194, 0.1656, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1271, 0.1383, 0.1409, 0.1640, 0.2325, 0.1972, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2626, 0.0625, 0.1251, 0.1030, 0.1239, 0.1378, 0.1851, 0.0000,
               0.0000, 0.0000],
              [0.0567, 0.2622, 0.1431, 0.1534, 0.1113, 0.1113, 0.0855, 0.0764,
               0.0000, 0.0000],
              [0.0485, 0.1625, 0.0394, 0.0898, 0.1105, 0.0610, 0.0450, 0.1730,
               0.2703, 0.0000],
              [0.1228, 0.1065, 0.0938, 0.0972, 0.0818, 0.0811, 0.0964, 0.0876,
               0.1195, 0.1134]],
    
             [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2949, 0.7051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1430, 0.4519, 0.4052, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.0776, 0.4069, 0.2460, 0.2695, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1079, 0.2525, 0.2378, 0.2130, 0.1888, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1599, 0.1222, 0.1519, 0.2000, 0.1937, 0.1723, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2235, 0.0925, 0.1991, 0.1290, 0.1052, 0.1105, 0.1401, 0.0000,
               0.0000, 0.0000],
              [0.0621, 0.1984, 0.2054, 0.0995, 0.0907, 0.1398, 0.1543, 0.0499,
               0.0000, 0.0000],
              [0.2403, 0.0418, 0.0847, 0.1005, 0.1029, 0.0734, 0.0883, 0.2401,
               0.0281, 0.0000],
              [0.0573, 0.1414, 0.0808, 0.1140, 0.1000, 0.1079, 0.0647, 0.0806,
               0.1548, 0.0985]]],
    
    
            [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1537, 0.8463, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2720, 0.3493, 0.3788, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2141, 0.2872, 0.2120, 0.2866, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2953, 0.2887, 0.1711, 0.2449, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2953, 0.2887, 0.1711, 0.2449, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2953, 0.2887, 0.1711, 0.2449, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2953, 0.2887, 0.1711, 0.2449, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2953, 0.2887, 0.1711, 0.2449, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2953, 0.2887, 0.1711, 0.2449, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000]],
    
             [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.4233, 0.5767, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3252, 0.3683, 0.3066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1960, 0.2759, 0.2367, 0.2914, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3322, 0.2703, 0.2373, 0.1602, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3322, 0.2703, 0.2373, 0.1602, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3322, 0.2703, 0.2373, 0.1602, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3322, 0.2703, 0.2373, 0.1602, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3322, 0.2703, 0.2373, 0.1602, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3322, 0.2703, 0.2373, 0.1602, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000]]],
    
    
            [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.5231, 0.4769, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2591, 0.2865, 0.4544, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2592, 0.2521, 0.2295, 0.2592, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1800, 0.1978, 0.2097, 0.1800, 0.2326, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1590, 0.1778, 0.1523, 0.1590, 0.1932, 0.1587, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.0879, 0.0908, 0.2131, 0.0879, 0.1199, 0.1335, 0.2669, 0.0000,
               0.0000, 0.0000],
              [0.1051, 0.1155, 0.1225, 0.1051, 0.1359, 0.1285, 0.1516, 0.1359,
               0.0000, 0.0000],
              [0.0993, 0.1049, 0.0530, 0.0993, 0.1339, 0.1296, 0.1068, 0.1339,
               0.1394, 0.0000],
              [0.0916, 0.0899, 0.0730, 0.0916, 0.1239, 0.1749, 0.0961, 0.1239,
               0.1352, 0.0000]],
    
             [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.4885, 0.5115, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.3038, 0.4251, 0.2711, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.2085, 0.2726, 0.3104, 0.2085, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1569, 0.1347, 0.3346, 0.1569, 0.2170, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1881, 0.1534, 0.1473, 0.1881, 0.1681, 0.1549, 0.0000, 0.0000,
               0.0000, 0.0000],
              [0.1039, 0.0752, 0.2323, 0.1039, 0.1386, 0.2061, 0.1401, 0.0000,
               0.0000, 0.0000],
              [0.0889, 0.0763, 0.1896, 0.0889, 0.1230, 0.1846, 0.1256, 0.1230,
               0.0000, 0.0000],
              [0.0926, 0.1408, 0.0801, 0.0926, 0.1151, 0.0988, 0.1790, 0.1151,
               0.0858, 0.0000],
              [0.0998, 0.1993, 0.1475, 0.0998, 0.0883, 0.0959, 0.0732, 0.0883,
               0.1080, 0.0000]]]], grad_fn=<SoftmaxBackward>)
    torch.Size([5, 2, 10, 10])



```python
attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)

print(attn_values.shape)
```

    torch.Size([5, 2, 10, 4])


### **전체 코드**


```python
class MultiheadAttention(nn.Module):
  def __init__(self):
    super(MultiheadAttention, self).__init__()

    # Q, K, V learnable matrices
    self.w_q = nn.Linear(d_model, d_model)
    self.w_k = nn.Linear(d_model, d_model)
    self.w_v = nn.Linear(d_model, d_model)

    # Linear transformation for concatenated outputs
    self.w_0 = nn.Linear(d_model, d_model)

  def forward(self, q, k, v, mask=None):
    batch_size = q.shape[0]

    q = self.w_q(q)  # (B, L, d_model)
    k = self.w_k(k)  # (B, L, d_model)
    v = self.w_v(v)  # (B, L, d_model)

    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)

    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)

    attn_values = self.self_attention(q, k, v, mask=mask)  # (B, num_heads, L, d_k)
    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)

    return self.w_0(attn_values)

  def self_attention(self, q, k, v, mask=None):
    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)

    if mask is not None:
      mask = mask.unsqueeze(1)  # (B, 1, L, L) or  (B, 1, 1, L)
      attn_scores = attn_scores.masked_fill_(mask == False, -1*inf)

    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)

    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)

    return attn_values
```


```python
multihead_attn = MultiheadAttention()

outputs = multihead_attn(batch_emb, batch_emb, batch_emb, mask=mask)  # (B, L, d_model)
```


```python
print(outputs)
print(outputs.shape)
```

    tensor([[[ 5.3039e-02,  2.8774e-01, -2.8584e-01,  1.3929e-01,  5.0676e-02,
              -1.4761e-02, -3.1077e-01, -1.4691e-01],
             [ 1.0387e-01,  1.8664e-01, -4.7584e-01,  2.1127e-01, -8.8178e-02,
               1.2211e-03, -3.2114e-01, -1.6862e-01],
             [-4.1654e-02,  1.3543e-01, -3.3242e-01,  1.9777e-01, -1.0637e-01,
              -3.3832e-02, -3.4562e-01, -1.5285e-01],
             [-1.0297e-01,  1.5745e-01, -2.1280e-01,  1.5691e-01, -1.0759e-01,
              -2.3487e-02, -2.9052e-01, -1.5205e-01],
             [-2.5156e-01,  1.1610e-01, -2.2453e-01,  4.1025e-02, -1.9633e-01,
              -1.7192e-01, -1.5224e-01, -2.2940e-01],
             [ 2.1537e-01,  5.1961e-01, -3.8899e-01,  4.4844e-02,  1.5645e-01,
              -2.0347e-02, -1.6964e-01, -2.3751e-01],
             [ 4.9815e-02,  3.9916e-01, -3.3118e-01,  9.4705e-02,  8.3534e-02,
              -1.9297e-02, -2.4476e-01, -2.3443e-01],
             [ 1.7586e-01,  5.2463e-01, -3.9441e-01,  2.9734e-02,  1.6427e-01,
              -4.3802e-02, -1.8395e-01, -2.5989e-01],
             [ 1.5798e-01,  5.1115e-01, -4.0195e-01,  1.8304e-02,  1.4554e-01,
              -6.2249e-02, -1.7359e-01, -2.6772e-01],
             [ 1.5798e-01,  5.1115e-01, -4.0195e-01,  1.8304e-02,  1.4554e-01,
              -6.2249e-02, -1.7359e-01, -2.6772e-01]],
    
            [[ 2.5216e-01,  7.9940e-01,  1.1118e-01,  2.0045e-01,  3.9805e-01,
               8.1003e-05,  2.3800e-02, -2.4669e-01],
             [ 3.9712e-01,  7.1531e-01, -3.1629e-01,  1.6096e-01,  2.2892e-01,
               1.2128e-01, -1.0120e-01, -2.8865e-01],
             [ 4.1143e-01,  4.2873e-01, -3.2860e-01,  2.5312e-01,  1.8284e-03,
               1.3516e-01, -1.2918e-01, -1.4889e-01],
             [-1.4827e-01,  1.4460e-01, -1.1646e-01,  2.1842e-01, -1.9006e-01,
               9.6086e-02, -4.2974e-02, -2.0911e-01],
             [-4.5125e-02,  2.9934e-01, -1.8812e-01,  1.6568e-01, -3.0796e-02,
               5.2050e-02, -1.4093e-01, -2.1879e-01],
             [ 3.7276e-02,  4.4109e-01, -1.2659e-01,  1.6741e-01,  1.0443e-01,
              -7.0949e-02, -1.5421e-01, -2.2022e-01],
             [ 3.7276e-02,  4.4109e-01, -1.2659e-01,  1.6741e-01,  1.0443e-01,
              -7.0949e-02, -1.5421e-01, -2.2022e-01],
             [ 3.7276e-02,  4.4109e-01, -1.2659e-01,  1.6741e-01,  1.0443e-01,
              -7.0949e-02, -1.5421e-01, -2.2022e-01],
             [ 3.7276e-02,  4.4109e-01, -1.2659e-01,  1.6741e-01,  1.0443e-01,
              -7.0949e-02, -1.5421e-01, -2.2022e-01],
             [ 3.7276e-02,  4.4109e-01, -1.2659e-01,  1.6741e-01,  1.0443e-01,
              -7.0949e-02, -1.5421e-01, -2.2022e-01]],
    
            [[-7.8918e-01,  1.0069e-01,  3.5113e-01, -3.5184e-01,  3.2537e-02,
              -3.2928e-01, -5.5531e-01, -1.5919e-02],
             [-1.4474e-01,  4.7575e-01,  1.2882e-01, -3.0431e-01,  1.6060e-01,
               1.6184e-02, -2.3185e-01, -8.4732e-02],
             [ 6.2410e-02,  2.6053e-01, -4.0261e-01,  4.8418e-01, -8.9071e-02,
               4.0704e-01, -3.0344e-01, -2.9453e-01],
             [-1.4581e-01,  2.5887e-01, -1.5301e-01, -8.4256e-02, -3.5355e-02,
               4.3243e-02, -2.2128e-01, -1.7196e-01],
             [-7.8130e-02,  3.1167e-01, -2.3772e-01, -6.4559e-03, -2.0114e-02,
               8.3518e-02, -1.5691e-01, -2.3758e-01],
             [-2.2968e-01,  1.9848e-01, -2.1891e-01,  3.0800e-02, -8.0161e-02,
              -1.4557e-03, -1.9712e-01, -2.3503e-01],
             [-7.9442e-02,  2.2845e-01, -3.7626e-01,  3.5389e-01, -8.1978e-02,
               2.3505e-01, -2.6957e-01, -3.0647e-01],
             [-3.6941e-01,  1.7102e-01, -7.4058e-02, -5.9366e-02, -9.0756e-02,
              -5.3732e-02, -1.3515e-01, -2.1331e-01],
             [-8.5861e-02,  3.6653e-01, -2.2073e-01,  1.2096e-01,  7.5610e-02,
               5.9410e-02, -2.3421e-01, -2.4194e-01],
             [-7.6437e-03,  3.8688e-01, -2.2642e-01,  6.5405e-02,  5.1432e-02,
               1.3267e-01, -1.5831e-01, -2.2531e-01]],
    
            [[-3.2183e-01,  5.1538e-01, -2.6286e-02, -1.6759e-01,  2.9642e-01,
              -3.1628e-01,  2.0068e-02, -2.8782e-01],
             [-1.8248e-02,  5.1236e-01, -1.3682e-01, -3.5730e-02,  1.9574e-01,
              -6.2725e-02, -1.9595e-01, -1.9365e-01],
             [-7.8095e-02,  4.0407e-01, -1.1295e-01, -1.3954e-02,  7.5328e-02,
               1.7142e-02, -1.5176e-01, -1.9389e-01],
             [-1.5868e-01,  3.2602e-01, -8.2980e-02,  3.7021e-02,  4.8315e-02,
               2.3331e-03, -2.0436e-01, -1.8203e-01],
             [-1.7196e-01,  3.2032e-01, -8.9658e-02,  3.6743e-02,  3.9270e-02,
              -5.7634e-02, -1.4848e-01, -2.0183e-01],
             [-1.7196e-01,  3.2032e-01, -8.9658e-02,  3.6743e-02,  3.9270e-02,
              -5.7634e-02, -1.4848e-01, -2.0183e-01],
             [-1.7196e-01,  3.2032e-01, -8.9658e-02,  3.6743e-02,  3.9270e-02,
              -5.7634e-02, -1.4848e-01, -2.0183e-01],
             [-1.7196e-01,  3.2032e-01, -8.9658e-02,  3.6743e-02,  3.9270e-02,
              -5.7634e-02, -1.4848e-01, -2.0183e-01],
             [-1.7196e-01,  3.2032e-01, -8.9658e-02,  3.6743e-02,  3.9270e-02,
              -5.7634e-02, -1.4848e-01, -2.0183e-01],
             [-1.7196e-01,  3.2032e-01, -8.9658e-02,  3.6743e-02,  3.9270e-02,
              -5.7634e-02, -1.4848e-01, -2.0183e-01]],
    
            [[ 3.6912e-02,  5.7633e-01,  5.4935e-02,  4.2949e-02,  2.0754e-01,
               2.1005e-01, -6.7775e-03, -2.0044e-01],
             [ 8.5174e-02,  4.5861e-01, -2.9692e-01,  7.3897e-02,  8.0098e-02,
               2.1827e-01, -5.0356e-02, -2.7975e-01],
             [ 1.9832e-01,  2.4814e-01, -3.5394e-01,  1.5729e-01, -1.2346e-01,
               2.4484e-01, -1.3469e-01, -1.4521e-01],
             [ 7.6393e-02,  2.7421e-01, -2.9814e-01,  1.5028e-01, -7.6509e-02,
               2.5854e-01, -1.3586e-01, -1.8659e-01],
             [ 4.9465e-02,  3.7245e-01, -2.1815e-01,  1.5148e-01,  3.3165e-02,
               1.9708e-01, -1.7732e-01, -2.1327e-01],
             [ 3.2809e-02,  3.4898e-01, -1.4356e-01,  1.5431e-01,  1.1798e-02,
               1.3426e-01, -1.2752e-01, -1.9553e-01],
             [-1.2399e-01,  1.9456e-01, -1.4644e-01,  2.5791e-01, -8.0899e-02,
               1.0080e-01, -2.6128e-01, -1.8909e-01],
             [-6.3265e-02,  2.9993e-01, -1.3608e-01,  2.1602e-01,  1.6934e-03,
               1.0468e-01, -2.3537e-01, -2.0822e-01],
             [-1.4606e-01,  3.2051e-01, -2.2598e-02,  9.1089e-02,  2.2396e-02,
               3.1288e-02, -1.5243e-01, -1.9624e-01],
             [-1.4114e-01,  3.1697e-01, -2.3063e-02,  6.9389e-02,  2.6310e-02,
               1.0185e-02, -1.6645e-01, -1.7984e-01]]], grad_fn=<AddBackward0>)
    torch.Size([5, 10, 8])


### **Encoder-Decoder attention**

Query, key, value만 달라질 뿐 구현은 동일합니다.  
Decoder에 들어갈 batch만 별도 구현하겠습니다.


```python
trg_data = [
  [33, 11, 49, 10],
  [88, 34, 5, 29, 99, 45, 11, 25],
  [67, 25, 15, 90, 54, 4, 92, 10, 46, 20, 88 ,19],
  [16, 58, 91, 47, 12, 5, 8],
  [71, 63, 62, 7, 9, 11, 55, 91, 32, 48]
]

trg_data, trg_max_len = padding(trg_data)
```

    100%|██████████| 5/5 [00:00<00:00, 12977.43it/s]

    Maximum sequence length: 12


    



```python
# S_L: source maximum sequence length, T_L: target maximum sequence length
src_batch = batch  # (B, S_L)
trg_batch = torch.LongTensor(trg_data)  # (B, T_L)

print(src_batch.shape)
print(trg_batch.shape)
```

    torch.Size([5, 10])
    torch.Size([5, 12])



```python
src_emb = embedding(src_batch)  # (B, S_L, d_w)
trg_emb = embedding(trg_batch)  # (B, T_L, d_w)

print(src_emb.shape)
print(trg_emb.shape)
```

    torch.Size([5, 10, 8])
    torch.Size([5, 12, 8])


`src_emb`를 encoder에서 나온 결과, 그리고 `trg_emb`를 masked multi-head attention 후 결과로 가정합니다.


```python
q = w_q(trg_emb)  # (B, T_L, d_model)
k = w_k(src_emb)  # (B, S_L, d_model)
v = w_v(src_emb)  # (B, S_L, d_model)

batch_size = q.shape[0]
d_k = d_model // num_heads

q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)
k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)
v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)

q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)
k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)
v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)

print(q.shape)
print(k.shape)
print(v.shape)
```

    torch.Size([5, 2, 12, 4])
    torch.Size([5, 2, 10, 4])
    torch.Size([5, 2, 10, 4])



```python
attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L)
attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)

print(attn_dists.shape)
```

    torch.Size([5, 2, 12, 10])



```python
attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)

print(attn_values.shape)
```

    torch.Size([5, 2, 12, 4])


Masked multi-head attention 후 나온 결과와 동일한 shape를 가지며 이후 layer에서 전체 연산도 동일하게 진행됩니다.


```python

```
