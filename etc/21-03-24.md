# 기본 개념 정리

* likelihood란 무엇인가 - 빨간 공과 파란 공이 들어있는 항아리 A, B가 있을 떄 p(파란공|항아리A) 은 항아리 A에서 공을 꺼냈을 때 그 공이 파란 공일 확률을 나타냄. 반대로 p(항아리 A|파란공)은 파란 공을 꺼냈을 때 그 공을 항아리 A에서 꺼냈을 확률을 나타냄. p(항아리 B|파란공)는 파란공이 항아리 B에서 나왔을 확률임. 이렇게 likelihood는 결과가 주어졌을 때(파란 공) 이 결과가 나오게 한 원인(항아리 A/B)의 확률 분포를 나타냄.

* likelihood와 deep learning은 무슨 관계인지? - deep learning 모델이 예측하여 내놓는 값이 바로 이 likelihood임. deep learning model에 파란 공을 input으로 넣으면 model은 output으로 이 파란 공이 항아리 A에서 나왔는지, 항아리 B에서 나왔는지에 대한 확률 분포를 내놓음. 이 중 확률이 가장 큰 것을 최종 결과로 선택한다. 

* maximum likelihood estimation이란 무엇인가? - 위에서 구한 최종 결과에서 ground truth들에 대한 확률 값들이 최대가 되도록 만드는 것이 maximum likelihood estimation이다. 이 때 ground truth에 대한 확률 값들을 모두 곱한 것을 likelihood 값으로 사용하고, 이 값이 최대가 되도록 한다. 그리고 여기에 log를 취해도 결과는 변하지 않으므로 log를 취해서 log likelihood를 최대화 한다. log를 취하는 경우 곱셈 연산이 덧셈 연산으로 바뀌기 때문에 연산 속도가 더 빨라지고, overflow 문제가 줄어들게 된다
수식을 살펴보면 로그 최대 가능도 추정은 cross entropy와 동치가 된다.

* entropy란 무엇인가 - entropy는 무질서도로 번역됨. entropy가 낮을수록 질서 정연하게 정리되어 있는 상태. entropy가 높을수록 뒤죽 박죽 섞여있는 무질서한 상태. 
dataset에 data가 등장할 가능성에(확률), 그 data를 표현(encoding)하는데 필요한 (최적의) bit 수를 곱한 뒤 평균을 내서 얻어낸 전체 dataset에 대한 최적의 encoding 결과 값.

* cross entropy란 무엇인가 - 여러가지 dataset(p, q)을 각각의 상태에 알맞은 최적의 방법으로 encoding할 수 있음. 이렇게 encoding 했을 때 나오는 것이 각각의 entropy 값. p dataset을 encoding할 때 썼던 encoding 방법을 q dataset을 encoding 하는데 적용해 볼 수 있음. 이렇게 다른 dataset을 위한 encoding 방법을 dataset에 적용해 보는 것을 cross-entropy라고 함. (entropy를 계산하는 수식에서는 한 dataset에 대한 확률만 사용했다면, cross-entropy를 계산할 때에는 p dataset의 data 확률과 q dataset의 data 등장 확률을 교차하여 사용함 - p의 확률 x q를 encoding 할 때의 bit 수 -> q를 encoding 했던 방식으로 p를 encoding 했을 때의 평균 길이를 의미하게 됨)

* cross entropy와 ML은 무슨 관련이 있는가? - ML에서는 사람이 encoding한 entropy(ground truth)와 기계가 encoding한 entropy(model output)를 비교한다. 기계가 사람과 완벽히 똑같은 encoding 결과를 내놓았다면 cross-entropy가 일반 entropy가 되고, 이떄의 값은 최적의 encoding 방법으로 encoding 했을 떄의 정보량을 나타내기 때문에 최소가 된다

* KL Diversions는 무엇인가? - KL Diversions는 cross-entropy에서 entropy를 뺀 것이다.  cross-entropy에서 최적의 정보량인 entropy를 뺌으로서 model이 최적의 encoder보다 부족한 부분을 나타낸 것이다. (정답과 model output 사이의 차이(오차)를 의미). KL Diversions를 0으로 만드는 것은 cross-entropy를 최소로 만드는 것과 동일하다.

* loss function이란 무엇인가 - ML model이 내놓는 결과와 ground truth를  비교하여 model의 output이 ground truth와 유사해 지도록 둘의 차이를 줄여 나가는 것이 model을 학습시키는 과정이다. 이 때 model의 output과 ground truth 사이의 차이를 계산하는 데 사용하는 것이 loss function이다

* gradient descent란 무엇인가 - loss function으로 값을 계산했으면, loss function의 값이 감소하는 방향으로 model을 학습시키게 된다. 이 때 사용하는 방법이 gradient descent 방법을 이용한다. 각 parameter에 대한 미분값들을 구한 뒤 이 값을 빼면 loss function의 값이 감소하는 방향으로 이동하게 된다.

* learning rate이란 무엇인가 - 미분 값을 빼줄 때 그냥 빼지 않고 learning rate를 미분 값에 곱한 뒤 빼준다. 이렇게 해서 뺄 값의 크기를 조절한다. 이것은 model의 학습 속도와 이동하는 거리를 조절하는 것을 의미한다.

참고 자료 - 홍원의 마스터님이 opentutorials에 올리셨던 강의 https://opentutorials.org/module/3653
